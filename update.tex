\documentclass[11pt,a4paper]{article}

% \documentclass{article}
% \usepackage{arxiv}

\setlength{\emergencystretch}{3.3em}

\usepackage{authblk}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
%\usepackage[bookmarksnumbered]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{cleveref}       % smart cross-referencing
%\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
%\usepackage{natbib}
\usepackage{doi}

% %%%hyperlinks

% \usepackage[dvipsnames]{xcolor}

% \newcommand\myshade{85}
% \colorlet{mylinkcolor}{violet}
% \colorlet{mycitecolor}{YellowOrange}
% \colorlet{myurlcolor}{Aquamarine}

% \hypersetup{
%   linkcolor  = mylinkcolor!\myshade!black,
%   citecolor  = mycitecolor!\myshade!black,
%   urlcolor   = myurlcolor!\myshade!black,
%   colorlinks = true,
% }

% %%%
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,      
  urlcolor=cyan,
  % pdftitle={An Example},
  % pdfpagemode=FullScreen,
}

\urlstyle{same}

%%%


\usepackage{mathrsfs}
\usepackage{mathtools}

\usepackage{algpseudocode}

\usepackage{stmaryrd}
% for \varominus \varoslash \lBrack \rBrack

\DeclarePairedDelimiter\Brackets{\llbracket}{\rrbracket}
\DeclarePairedDelimiter\SemiOpenBrackets{\llbracket}{\rrparenthesis}

\usepackage{framed}

\usepackage{amsmath,amssymb}

\usepackage[amsmath,framed,thmmarks]{ntheorem}

\makeatletter
\newtheoremstyle{mybreak}%
  {\item[\rlap{\vbox{\hbox{\hskip\labelsep \theorem@headerfont
          ##1\ ##2\theorem@separator}\hbox{\strut}}}]}%
  {\item[\rlap{\vbox{\hbox{\hskip\labelsep \bf{\theorem@headerfont}
%           ##1\ ##2\ (##3)\theorem@separator}\hbox{\strut}}}]}% DELETED
          ##1\ ##2:\ ##3\theorem@separator}\hbox{\strut}}}]}% NEW
  \makeatother

% \theoremstyle{mybreak}
% %\theoremheaderfont{\normalfont\smallskip\scriptsize\scshape}
% \theoremheaderfont{\normalfont\smallskip\scriptsize\bfseries}
% \theorembodyfont{\normalfont\scriptsize}
% \theoremindent0.5cm
% \newframedtheorem{algorithm}{Algorithm}

% \theoremstyle{mybreak}
% \theoremheaderfont{\normalfont}  
% \theorembodyfont{\normalfont}
% \theoremseparator{\smallskip}
% \theoremprework{\bigskip\hrule\smallskip}
% %\theorempostwork{\bigskip\hrule\bigskip}
% \newtheorem{algorithm}{Algorithm}

\theoremstyle{break}
\theorembodyfont{\normalfont}
%\theoremheaderfont{\normalfont}
\theoremseparator{}
\theoremprework{\bigskip\noindent\rule{\textwidth}{0.08em}\smallskip}
%\theoremprework{\bigskip\hrule\vspace{0.3ex}\hrule\bigskip}
\theorempostwork{\rule{\textwidth}{0.08em}\bigskip}
%\theorempostwork{\bigskip\hrule\vspace{0.3ex}\hrule\bigskip}
\newtheorem*{algorithm}{Algorithm}

\usepackage{chngcntr}
\counterwithin{equation}{section}

\newcounter{dummy}
\numberwithin{dummy}{section}

\newtheorem{theorem}{Theorem}%[section]

\theorembodyfont{\rmfamily\slshape}
\theoremstyle{plain}
\theoremseparator{.}
\newtheorem{result}{Result}[section]

\theorembodyfont{\normalfont}
\theoremstyle{plain}
\theoremseparator{.}
\theoremprework{\bigskip\hrule\bigskip}
\theorempostwork{\smallskip\hrule\bigskip}
\newtheorem*{problem}{Problem}

% definition
%\theoremstyle{break}
%\theoremseparator{.}
%\theoremprework{\bigskip\hrule\bigskip}
%\theorempostwork{\smallskip\hrule\bigskip}
%\newtheorem*{def}{Definition}

\theoremstyle{plain}
\theoremseparator{.}
\theoremprework{\bigskip}
\theorempostwork{\smallskip}
\newtheorem{definition}{Definition}[section]

\theoremstyle{plain}
\theoremseparator{.}
\theoremprework{\bigskip}
\theorempostwork{\smallskip}
\newtheorem{example}{Example}[section]

\AfterEndEnvironment{definition}{\noindent\ignorespaces}

\theoremstyle{plain}
\theoremseparator{.}
\theoremprework{\bigskip}
\theorempostwork{\smallskip}
\newtheorem{remark}{Remark}[section]

% A proof environment.
% This is done with the extra commands made available from ntheorem
% \theoremstyle{nonumberplain}
\makeatletter
\newtheoremstyle{MyNonumberplain}%
  {\item[\theorem@headerfont\hskip\labelsep ##1\theorem@separator]}%
  {\item[\theorem@headerfont\hskip\labelsep ##3\theorem@separator]}
\makeatother
\theoremstyle{MyNonumberplain}
\theoremheaderfont{\normalfont\itshape}
\theorembodyfont{\normalfont}
\theoremseparator{.}
\theoremsymbol{\ensuremath{\square}}
\newtheorem{proof}{Proof}

\usepackage{comment}

% Uncomment to override  the `A preprint' in the header
% \renewcommand{\headeright}{}
% \renewcommand{\undertitle}{}
%\renewcommand{\shorttitle}{Integrated Hessian}

%\documentclass[final,times,5p,7pt,twocolumn,sort&compress,fleqn]{elsarticle}
%\usepackage[numbers]{natbib}

\pretolerance=2000

% to correct space before and after delimiters
\usepackage{mleftright}
\mleftright 

%page geometry
% \usepackage[colorlinks,linkcolor=IVAIteal,citecolor=IVAIpurple,urlcolor=IVAIpurple]{hyperref}
 % \usepackage[lmargin=2cm,rmargin=2cm,tmargin=1.25cm,bmargin=1.25cm,includefoot,includehead]{geometry}

% typography
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% % Serif Times fonts
% \renewcommand{\rmdefault}{ptm}

% \usepackage{utopia}
% \usepackage{tgschola}
% \usepackage{tgtermes}

\usepackage[xspace]{ellipsis}
% \usepackage{pifont}% http://ctan.org/pkg/pifont
% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%

% \usepackage{charter}

\usepackage{inconsolata} % for tighter \tt (monospace fonts)

\usepackage{xfrac}

% \usepackage{titling}
% \pretitle{\begin{center}\Large\fontfamily{phv}\fontseries{bf}\selectfont}
% \posttitle{\end{center}}
% \preauthor{\begin{center}\large}
% \postauthor{\end{center}}
% \predate{\begin{center}\footnotesize}
% \postdate{\end{center}}
% \setlength{\droptitle}{-1cm}


%\usepackage{charter}

%% Sans-serif Arial-like fonts (Helvetica)
%\renewcommand{\sfdefault}{phv} 

%text structures
\usepackage[inline,shortlabels]{enumitem}
\usepackage{tabu}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{eurosym}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage{relsize}

\usepackage{placeins}
\usepackage{afterpage}

\usepackage{subcaption}
\captionsetup{font=footnotesize}

\usepackage{float}
%\usepackage[parfill]{parskip}
%\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tabulary}

% \usepackage{mathrsfs}
% \usepackage{mathtools}

% \usepackage{amsmath,amsthm}
% \interdisplaylinepenalty=2500

\usepackage{amssymb}
%\usepackage{bbm}
\usepackage{bm}
%\usepackage{upgreek}

\usepackage{ifthen} % provides \ifthenelse test  
\usepackage{xifthen}

\usepackage[active]{srcltx}

\usepackage{stmaryrd} % provides double brackets

\usepackage{xspace}

\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\rk}{rank}
\DeclareMathOperator{\col}{Col}

\renewcommand{\arraystretch}{1.3} 

\newcommand\undermat[2]{%
  \makebox[0pt][l]{$\smash{\underbrace{\phantom{%
    \begin{bmatrix}#2\end{bmatrix}}}_{\text{$#1$}}}$}#2}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{{e.\,g}\onedot} \def\Eg{{E.g}\onedot}
\def\ie{{i.\,e}\onedot} \def\Ie{{I.e}\onedot}
\def\cf{{cf}\onedot} \def\Cf{{C.f}\onedot}
\def\etc{{etc}\onedot}
\def\vs{{vs}\onedot}
\def\wrt{w.\,r.\,t\onedot}
\def\dof{d.\,o.\,f\onedot}
\def\etal{{et al}\onedot}
\def\almoste{a.\,e\onedot}
\makeatother

% to stretch bmatrices, pmatrices, and the like
% \makeatletter
% \renewcommand*\env@matrix[1][\arraystretch]{%
%   \edef\arraystretch{#1}%
%   \hskip -\arraycolsep
%   \let\@ifnextchar\new@ifnextchar
%   \array{*\c@MaxMatrixCols c}}
% \makeatother

% \usepackage{array}
% \renewcommand{\arraystretch}{1.5}

%\usepackage{cmdtrack}
%\usepackage{refcheck}

% \newcommand*{\expect}{\mathsf{E}}
% \newcommand*{\prob}{\mathsf{P}}
% \newcommand{\IG}{\Xib_{\mathrm{IG}}}
% \newcommand{\Psib}{\bm{\Uppsi}}
% \newcommand{\Vm}{V^-}
% \newcommand{\Vpm}{V^{\pm}}
% \newcommand{\Vp}{V^+}
% \newcommand{\Win}{\M{W}_{\textrm{in}}}
% \newcommand{\Wout}{\M{W}_{\textrm{out}}}
% \newcommand{\Xib}{\bm{\Upxi}}
% \newcommand{\Zplus}{\mathbb{Z}_{\geqslant{0}}}
% \newcommand{\agm}[5][f]{\tilde{\mu}^{(#1)}_{#2,#3;#4,#5}}
% \newcommand{\gm}[5][f]{\mu^{(#1)}_{#2,#3;#4,#5}}
% \newcommand{\lambdab}{\bm{\uplambda}}
% \newcommand{\lambpm}{\lambda_{\pm}}
% \newcommand{\pcr}{\tens{C}}
% \newcommand{\phibt}{\tilde{\bm{\upphi}}}
% \newcommand{\phib}{\bm{\upphi}}
% \newcommand{\phit}{\tilde{\phi}}
% \newcommand{\pro}[2][P]{\M{#1}_{#2}^\perp}
% \newcommand{\psib}{\bm{\uppsi}}
% \newcommand{\tde}{\tilde\de}
% \newcommand{\truede}{\underline{\de}}
% \newcommand{\truetde}{\underline{\tde}}
%\newcommand{\IG}{{\bm{\upgamma}}}
%\newcommand{\J}{\mathbf{J}}
%\newcommand{\betabold}{\boldsymbol{\upbeta}}
%\newcommand{\xib}{\bm{\xi}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\0}{\M{0}}
\newcommand{\IGs}{u}
\newcommand{\IG}{\bm{\nabla}^{\mathrm{int}}}
\newcommand{\IH}{\bm{\mathsf{D}\nabla}^{\mathrm{int}}}
\newcommand{\M}[1]{\mathbf{#1}}
\newcommand{\Mt}[1]{\tilde{\M{#1}}}
\newcommand{\Mr}[1]{{\M{#1}}^{\mathrm{red}}}
\newcommand{\Mrt}[1]{{\tilde{\M{#1}}}^{\mathrm{red}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Nzero}{\Z^+}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\top}
\newcommand{\comp}[2]{{\left\llbracket #1\right\rrbracket}_{#2}}
\newcommand{\dderiv}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\deq}{\mathrel{\stackrel{\scriptscriptstyle\Delta}{=}}}
\newcommand{\deriv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\der}[1]{\bm{\mathsf{D}}{#1}}
\newcommand{\de}{\boldsymbol{\theta}}
\newcommand{\emphbf}[1]{\emph{\textbf{#1}\xspace}}
\newcommand{\entry}[3]{{\left\llbracket#1\right\rrbracket}_{#2 #3}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\grad}[1]{\bm{\nabla}{#1}}
\newcommand{\hess}[1]{\bm{\mathsf{D}\nabla}{#1}}
\newcommand{\sderiv}[2]{{\partial #1}/{\partial #2}}
\newcommand{\ud}{\,\mathrm d}
\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\eve}[2]{\mathbf{e}^{(#1)}_{#2}}

\newcommand{\Mat}[2]{%
  \ifthenelse{\equal{#2}{1}}
  {\R^{#1}}
  {\R^{#1 \times #2}}
}

\newcommand{\derivtwo}[3]{%
  \ifthenelse{\equal{#2}{#3}}
  {\frac{\partial^2#1}{\partial #2^2}}
  {\frac{\partial^2#1}{\partial #2 \partial #3}}
}

\newcommand{\dderivtwo}[3]{%
  \ifthenelse{\equal{#2}{#3}}
  {\dfrac{\partial^2#1}{\partial #2^2}}
  {\dfrac{\partial^2#1}{\partial #2 \partial #3}}
}

% \newcommand{\dderivtwo}[3]{%
%   \ifthenelse{\isempty{#3}}
%   {\dfrac{\partial^2#1}{\partial #2^2}}
%   {\dfrac{\partial^2#1}{\partial #2 \partial #3}}
% }

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sgn}{sgn}

\newcommand{\delcol}[2]{\M{#1} \varominus {\mathrm{col}}({#2})}
%\newcommand{\delcol}[2]{\M{#1}^{:, -#2}}
\newcommand{\delrow}[2]{\M{#1} \varominus \mathrm{row}(#2)}
%\newcommand{\delrow}[2]{\M{#1}^{-#2,:}}
\newcommand{\sel}[2]{\M{J}_{#1,#2}}
\newcommand{\zercol}[2]{\M{#1} \varoslash \mathrm{col}(#2)}
%\newcommand{\zercol}[2]{\M{#1}^{:,\0 \rightarrow #2}}
\newcommand{\zerrow}[2]{\M{#1} \varoslash \mathrm{row}(#2)}
%\newcommand{\zerrow}[2]{\M{#1}^{#2 \leftarrow \0^\T,:}}

\usepackage[active]{srcltx}

\usepackage{colonequals}


% generating plots

\usepackage{pgfplots}
\pgfplotsset{compat=newest}

\setlist{nosep} 

% \newtheorem{theorem}{Theorem}%[section]
% \newtheorem*{theorem*}{Theorem}

\newenvironment{changemargin}[2]{%
  \begin{list}{}{%
      \setlength{\topsep}{0pt}%
      \setlength{\leftmargin}{#1}%
      \setlength{\rightmargin}{#2}%
      \setlength{\listparindent}{\parindent}%
      \setlength{\itemindent}{\parindent}%
      \setlength{\parsep}{\parskip}%
    }%
  \item[]}
  {\end{list}}

\newenvironment{keywords}{%
  \begingroup
  \def\and{\unskip\space\textperiodcentered\space\ignorespaces}
  \begin{changemargin}{\leftmargin}{\leftmargin}
    \small\noindent\emph{Keywords}:}
  {\end{changemargin}
  \endgroup
}

\begin{document}

\title{A Note on the Incremental SVD}

\date{}

\author{}

% \affil{%
% Insight Via Artificial Intelligence Pty Ltd \\ Suite 811, 147 Pirie Street \\ Adelaide, SA 5000}

\maketitle

% \begin{abstract}
%   The mathematical formulation of the integrated Hessian method from machine learning involves a matrix whose entries contain integral expressions. Here we present a formula for the integrated Hessian matrix that uses single integrals over a line segment. This replaces the original expression for the same matrix that uses double integrals over a square domain.
% \end{abstract}

% \begin{keywords}
%   integrated gradient \and integrated Hessian \and single integral \and double integral
% \end{keywords}

% \title{A Note on the Integrated Hessian}

% \maketitle

\section{Updating}

\subsection{The method}

Let $\R$ denote the set of real numbers, and let $\Mat{m}{n}$ denote the set of $m \times n$ matrices with entries in $\R$.  We identify coordinate vectors in $\R^n$ with $n \times 1$ matrices in $\R^{n \times 1}$, or, what is the same, with length-$n$ column vectors with real entries.  Let $\M{X} \in \Mat{p}{q}$ be a matrix of rank $r \geq 1$. Let
\begin{equation}
  \label{eq:1}
  \M{X} = \M{U} \M{S} \M{V}^\T
\end{equation}
be the thin (or compact) SVD of $\M{X}$.  Here $\M{U} \in \Mat{p}{r}$ and $\M{V} \in \Mat{q}{r}$ have unit-norm, orthogonal columns, which can be expressed in symbols as
\begin{displaymath}
  \M{U}^\T \M{U} = \M{I}_r
  \quad
  \text{and}
  \quad
  \M{V}^\T \M{V} = \M{I}_r,
\end{displaymath}
with $\M{I}_r$ standing for the $r \times r$ identity matrix, and $\M{S} \in \Mat{r}{r}$ is a diagonal matrix with positive real entries in descending order,
\begin{math}
  \M{S} = \diag(s_1, \dots, s_r),
\end{math}
where
\begin{math}
  s_1 \geq \dots \geq s_r > 0.
\end{math} 
Let $\ve{c} \in \Mat{p}{1}$, and let $[\M{X}, \ve{c}] \in \Mat{p}{(q+1)}$ be the horizontal concatenation of $\M{X}$ and $\ve{c}$.  We are interested in finding a formula for the thin SVD of $[\M{X}, \ve{c}]$.

The key to obtaining the relevant expression is the representation
\begin{equation}
  \label{eq:2}
  [\M{X}, \ve{c}] = [\M{U}, \ve{p}] \M{K}
  \begin{bmatrix}
    \M{V} & \0
    \\
    \0^\T & 1
  \end{bmatrix}^\T,
\end{equation}
where $\ve{p} \in \R^p$ is given by
\begin{displaymath}
  \ve{p} =
  \begin{cases}
    \frac{\ve{c} - \M{U} \M{U}^\T \ve{c}}{\| \ve{c} - \M{U} \M{U}^\T \ve{c}\|} & \text{if $\ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0$,}
    \\
    \0 & \text{otherwise,}
  \end{cases}
\end{displaymath}
and $\M{K} \in \Mat{(r+1)}{(r+1)}$ is given by
\begin{displaymath}
  \M{K} =
  \begin{bmatrix}
    \M{S} & \M{U}^\T \ve{c} \\
    \0^\T & \| \ve{c} - \M{U} \M{U}^\T \ve{c} \|
  \end{bmatrix}.
\end{displaymath}
Let
\begin{equation}
  \label{eq:3}
  \M{K} = \M{C} \M{S}' \M{D}^\T
\end{equation}
be the thin SVD of $\M{K}$, with $\M{C} \in \Mat{(r+1)}{k}$, $\M{S}' \in \Mat{k}{k}$, and $\M{D} \in \Mat{(r+1)}{k}$, $k$~being the rank of $\M{K}$. Let $\M{U}' \in \Mat{p}{k}$ be given by
\begin{equation}
  \label{eq:4}
  \M{U}' = [\M{U}, \ve{p}] \M{C},
\end{equation}
and let $\M{V}' \in \Mat{(q+1)}{k}$ be given by
\begin{equation}
  \label{eq:5}
  \M{V}' =
  \begin{bmatrix}
    \M{V} & \0
    \\
    \0^\T & 1
  \end{bmatrix}
  \M{D}.
\end{equation}

% The pertinence of the representation \eqref{eq:2} lies in the following result.

\begin{result}
  \label{res:1}
  We have
  \begin{equation}
    \label{eq:6}
    [\M{X}, \ve{c}] = \M{U}' \M{S}' \M{V}'^\T,
  \end{equation}
  and this representation is the thin SVD of $[\M{X}, \ve{c}]$.
\end{result}

Before giving the proof, we make a comment. Since $\M{K}$ is an $(r+1) \times (r+1)$ matrix and $\M{S}$ is a submatrix of $\M{X}$ of rank $r$, the rank of $\M{K}$ is either $r$ or $r+1$. Since
\begin{displaymath}
  \det \M{K} = \det \M{S} \cdot 
  \| \ve{c} - \M{U} \M{U}^\T \ve{c} \| 
\end{displaymath}
and since
\begin{math}
  \det \M{S} \neq 0,
\end{math}
it follows that $\M{K}$ is invertible (that is, $\det \M{K} \neq 0$), and hence of rank $r+1$, if and only if
\begin{math}
  \ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0.
\end{math}
Clearly, the condition
\begin{math}
  \ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0
\end{math}
is equivalent to the condition
\begin{math}
  \ve{p} \neq \0.
\end{math}
Thus
\begin{equation}
  \label{eq:7}
  k =
  \begin{cases}
    r & \text{if $\ve{p} = \0$},
    \\
    r + 1 & \text{otherwise}.
  \end{cases}
\end{equation}

% Given a matrix $\M{A}$, let $\col(\M{X})$ denote the column space of $\M{A}$. Recalling that by $r$ we denote the rank of $\M{X}$, we have
% \begin{displaymath}
%     \rk [\M{X}, \ve{c}] =
%     \begin{cases}
%       r & \text{if $c \in \col(\M{X})$},
%       \\
%       r + 1 & \text{otherwise}.
%     \end{cases}
%   \end{displaymath}
%   Moreover, $c \notin \col(\M{X})$ if and only if
%   \begin{math}
%     \ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0.
%   \end{math}
%   We see that $[\M{X}, \ve{c}]$ and $\M{K}$ have the same rank, which is $r$ or $r+1$, and that this common rank is $r+1$ if and only if
%   \begin{math}
%     \ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0.
%   \end{math}

\begin{proof}[Proof of Result~\ref{res:1}]
  Formula~\eqref{eq:6} follows immediately from combining \eqref{eq:2}, \eqref{eq:3}, \eqref{eq:4}, and \eqref{eq:5}.  Since $\M{S}'$ is a diagonal matrix with positive elements in descending order, it remains only show that $\M{U}'$ and $\M{V}' $ have unit-norm, orthogonal columns, that is,
  \begin{math}
    \M{U}'^\T \M{U}' = \M{I}_k
  \end{math}
  and
  \begin{math}
    \M{V}'^\T \M{V}' = \M{I}_k.
  \end{math}

  Note that
  \begin{equation}
    \label{eq:8}
    \begin{split}
      \M{U}'^\T \M{U}' & = \M{C}^\T [\M{U}, \ve{p}]^\T [\M{U}, \ve{p}] \M{C} = \M{C}^\T
      \begin{bmatrix}
        \M{U}^\T \M{U} & \M{U}^\T \ve{p} \\
        \ve{p}^\T \M{U} & \| \ve{p} \|^2
      \end{bmatrix}
      \M{C}
      \\
      & = \M{C}^\T
      \begin{bmatrix}
        \M{I}_r & \M{U}^\T \ve{p} \\
        \ve{p}^\T \M{U} & \| \ve{p} \|^2
      \end{bmatrix}
      \M{C}.
    \end{split}
  \end{equation}
  If $\ve{p} = \0$, then, obviously, $\M{U}^\T \ve{p} = \0$.  If $\ve{p} \neq \0$, then
  \begin{displaymath}
    \M{U}^\T \ve{p}
    =  \frac{\M{U}^\T \ve{c} - \M{U}^\T \M{U} \M{U}^\T \ve{c}}
    {\| \ve{c} - \M{U}^\T \M{U} \ve{c}\|}
    =
    \frac{\M{U}^\T \ve{c} - \M{I}_r \M{U}^\T \ve{c}}
    {\| \ve{c} - \M{U}^\T \M{U} \ve{c}\|}
    =
    \frac{\M{U}^\T \ve{c} - \M{U}^\T \ve{c}}
    {\| \ve{c} - \M{U}^\T \M{U} \ve{c}\|}
    =
    \0.
  \end{displaymath}
  Thus in either case $\M{U}^\T \ve{p} = \0$. Moreover, $\ve{p}^\T \M{U} = (\M{U}^\T \ve{p})^\T = \0^\T$. Hence
  \begin{displaymath}
    \begin{bmatrix}
      \M{I}_r & \M{U}^\T \ve{p} \\
      \ve{p}^\T \M{U} & \| \ve{p} \|^2
    \end{bmatrix}
    =
    \begin{bmatrix}
      \M{I}_r & \0 \\
      \0^\T& \| \ve{p} \|^2
    \end{bmatrix}.
  \end{displaymath}
  We now consider two cases.
  \begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
  \item Suppose that $\ve{p} \neq \0$, in which case $k = r+1$. Then $\| \ve{p} \| = 1$ and
    \begin{equation}
      \label{eq:9}
      \begin{bmatrix}
        \M{I}_r & \M{U}^\T \ve{p}
        \\
        \ve{p}^\T \M{U} & \| \ve{p} \|^2
      \end{bmatrix}
      =
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T& 1
      \end{bmatrix}.
    \end{equation}
    Moreover,
    \begin{displaymath}
      \M{C}^\T
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T& 1
      \end{bmatrix}
      \M{C}
      =
      \M{C}^\T \M{I}_{r+1} \M{C}
      =
      \M{C}^\T \M{C}
      = \M{I}_k.
    \end{displaymath}
    Combining this with \eqref{eq:8} and \eqref{eq:9}, we see that
    \begin{math}
      \M{U}'^\T \M{U}' = \M{I}_k.
    \end{math}
  \item Suppose that $\ve{p} = \0$, in which case $k = r$. Then
    \begin{equation}
      \label{eq:10}
      \begin{bmatrix}
        \M{I}_r & \M{U}^\T \ve{p} \\
        \ve{p}^\T \M{U} & \| \ve{p} \|^2
      \end{bmatrix}
      =
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T & 0
      \end{bmatrix}.
    \end{equation}
    Moreover, the matrix $\M{K}$ now becomes
    \begin{displaymath}
      \M{K} =
      \begin{bmatrix}
        \M{S} & \M{U}^\T \ve{c} \\
        \0^\T & 0
      \end{bmatrix}.
    \end{displaymath}
    Since each column of $\M{K}$ has zero last entry and since the column space of $\M{C}$ coincides with the column space of $\M{K}$, it follows that each column of $\M{C}$ has zero last entry. In other words,
    \begin{equation}
      \label{eq:11}
      \M{C} =
      \begin{bmatrix}
        \M{C}^{1:r, :}
        \\
        \0^\T
      \end{bmatrix},
    \end{equation}
    where $\M{C}^{1:r, :} = \M{C}^{1:r,1:k} \in \Mat{r}{k}$.  Here we use the \textsc{Matlab}-style notation where $\M{C}^{1:r, :}$ denotes the submatrix of elements of $\M{C}$ that are in rows $1$ to $r$, and $\M{C}^{1:r,1:k}$ denotes the submatrix of elements of $\M{C}$ that are in rows $1$ to $r$ and in columns $1$ to $k$.
    % 
    % 
    % 
    % Here, given a matrix $\M{A}$ with at least $k$ rows and $k$ columns, we use the \textsc{Matlab}-style notation $\M{A}^{1:k,1:k}$ to denote the submatrix of elements of $\M{A}$ that are in rows $1$ to $k$ and in columns $1$ to $k$.
    %
    %
    %
    In view of \eqref{eq:11}, we have
    \begin{align*}
      \M{C}^\T
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T & 0
      \end{bmatrix}
                \M{C} & =
                        [(\M{C}^{1:r, :})^\T, \0]
                        \begin{bmatrix}
                          \M{I}_r & \0 \\
                          \0^\T & 0
                        \end{bmatrix}
                                  \begin{bmatrix}
                                    \M{C}^{1:r,:}
                                    \\
                                    \0^\T
                                  \end{bmatrix}
      \\ & =
           [(\M{C}^{1:r, :})^\T, \0]
           \begin{bmatrix}
             \M{C}^{1:r,:}
             \\
             \0^\T
           \end{bmatrix}
      = \M{C}^\T \M{C} = \M{I}_k.
    \end{align*}
    Combining this with \eqref{eq:8} and \eqref{eq:10}, we conclude that
    \begin{math}
      \M{U}'^\T \M{U}' = \M{I}_k.
    \end{math}
  \end{enumerate}
  Thus in either of the considered cases, 
  \begin{math}
    \M{U}'^\T \M{U}' = \M{I}_k.
  \end{math}
  
  Next note that
  \begin{align*}
    \M{V}'^\T \M{V}'
    &
      =
      \M{D}^\T
      \begin{bmatrix}
        \M{V} & \0
        \\
        \0^\T & 1
      \end{bmatrix}^\T
               \begin{bmatrix}
                 \M{V} & \0
                 \\
                 \0^\T & 1
               \end{bmatrix}
                        \M{D}
    \\
    & =
      \M{D}^\T
      \begin{bmatrix}
        \M{V}^\T & \0
        \\
        \0^\T & 1
      \end{bmatrix}
               \begin{bmatrix}
                 \M{V} & \0
                 \\
                 \0^\T & 1
               \end{bmatrix}
                        \M{D}
    \\
    & =
      \M{D}^\T
      \begin{bmatrix}
        \M{V}^\T \M{V} & \0
        \\
        \0^\T & 1
      \end{bmatrix}
               \M{D}
    \\
    & =
      \M{D}^\T
      \begin{bmatrix}
        \M{I}_r & \0
        \\
        \0^\T & 1
      \end{bmatrix}
               \M{D}
    \\
    & =
      \M{D}^\T \M{I}_{r+1} \M{D}
      = \M{D}^\T \M{D}
      = \M{I}_k.
  \end{align*}
  This completes the proof.
\end{proof}

\begin{remark}
  \label{rmk:1}
  Comparing \eqref{eq:3} and \eqref{eq:6}, we see that the SVD of $\M{K}$ and the SVD of $[\M{X}, \ve{c}]$ share the diagonal part $\M{S}'$.  It follows that $\M{K}$ and $[\M{X}, \ve{c}]$ have the same rank. Given a matrix $\M{A}$, let $\rk \M{A}$ denote the rank of $\M{A}$, and let $\col \M{A}$ denote the column space of $\M{A}$. Recalling that, for a given matrix $\M{A}$, the rank of $\M{A}$ equals the dimension of the column space of $\M{A}$, we readily see that
  \begin{equation}
    \label{eq:12}
    \rk [\M{X}, \ve{c}] =
    \begin{cases}
      r & \text{if $\ve{c} \in \col \M{X}$},
      \\
      r + 1 & \text{otherwise}.
    \end{cases}
  \end{equation}
  Let us remind ourselves that here $r$ denotes the rank of $\M{X}$.  A moment's reflection shows that $\ve{c} \notin \col \M{X}$ if and only if
  \begin{math}
    \ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0.
  \end{math}
  The latter condition holds if and only if
  \begin{math}
    \ve{p} \neq \0.
  \end{math}
  Thus \eqref{eq:12} is equivalent to \eqref{eq:7}. In other words, $k = r$ if $\ve{c}$ is in the linear span of the columns of $\M{X}$, and $k = r+1$ if $\ve{c}$ is not in the linear span of the columns of $\M{X}$.
\end{remark}


% We see that $[\M{X}, \ve{c}]$ and $\M{K}$ have the same rank, which is $r$ or $r+1$, and that this common rank is $r+1$ if and only if
% \begin{math}
%   \ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0.
% \end{math}

% We now consider two cases.
% \begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
% \item Suppose that $\rk [\M{X}, \ve{c}] = r + 1$, or, equivalently, that $\ve{c} - \M{U} \M{U}^\T \ve{c} \neq \0$. Since
%   \begin{displaymath}
%     \det \M{K} = \det \M{S} \cdot 
%     \| \ve{c} - \M{U} \M{U}^\T \ve{c} \| \neq 0,
%   \end{displaymath}
%   it follows that $\M{K}$ is invertible, and hence of rank $r + 1$. This implies that $\M{S}'$ has rank $r + 1$, and $\M{C}$ and $\M{D}$ are $(r+1) \times (r+1)$ orthogonal matrices.  Since $[\M{U}, \ve{p}] \in \Mat{p}{(r+1)}$ and
%   \begin{math}
%     \left[
%       \begin{smallmatrix}
%         \M{V} & \0
%         \\
%         \0^\T & 1
%       \end{smallmatrix}
%     \right] \in \Mat{(q+1)}{(r+1)}
%   \end{math}
%   have unit-norm, orthogonal columns, and since $\M{C}$ and $\M{D}$ are orthogonal, it follows that $\M{U}'$ and $\M{V}'$, defined in \eqref{eq:4}, have unit-norm, orthogonal columns. Thus \eqref{eq:6} gives the thin SVD of $[\M{X}, \ve{c}]$.
% \item Suppose that $\rk [\M{X}, \ve{c}] = r$, or, equivalently, that $\ve{p} = 0$. Then
%   \begin{displaymath}
%     \M{K} =
%     \begin{bmatrix}
%       \M{S} & \M{U}^\T \ve{c} \\
%       \0^\T & 0
%     \end{bmatrix}.
%   \end{displaymath}
%   The rank of $\M{K}$ is $r$, so $\M{S}' \in \Mat{r}{r}$ and $\M{C}, \M{D} \in \Mat{(r+1)}{r}$.  Since the last row of $\M{K}$ is a null vector, it follows that the last row of $\M{C}$ is also a null vector.  Thus
%   \begin{displaymath}
%     \M{C}
%     =
%     \begin{bmatrix}
%       \M{C}^{1:r, 1:r}
%       \\
%       \0^\T
%     \end{bmatrix},
%   \end{displaymath}
%   where $\M{C}^{1:r, 1:r}$ is an $r \times r$ orthogonal matrix. Consequently,
%   \begin{displaymath}
%     \M{U}' = [\M{U}, \ve{p}] \M{C}
%     = [\M{U}, \0]
%     \begin{bmatrix}
%       \M{C}^{1:r, 1:r}
%       \\
%       \0^\T
%     \end{bmatrix}
%     = \M{U} \M{C}^{1:r, 1:r}.
%   \end{displaymath}
%   Since $\M{D} \in \Mat{(r+1)}{r}$, we have
%   \begin{displaymath}
%     \M{D}
%     =
%     \begin{bmatrix}
%       \M{D}^{1:r,1:r}
%       \\
%       \M{D}^{r+1,1:r}
%     \end{bmatrix},
%   \end{displaymath}
%   and we also have
%   \begin{displaymath}
%     \M{V}' =
%     \begin{bmatrix}
%       \M{V} & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix}
%     \M{D}
%     =
%     \begin{bmatrix}
%       \M{V} & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix}
%     \begin{bmatrix}
%       \M{D}^{1:r,1:r}
%       \\
%       \M{D}^{r+1,1:r}
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       \M{V} \M{D}^{1:r,1:r}
%       \\
%       \M{D}^{r+1,1:r}
%     \end{bmatrix}.
%   \end{displaymath}

% \end{enumerate}

\subsection{Calculating $\M{U}$ and $\M{V}$ iteratively}

Suppose that the matrices $\M{U}$ and $\M{V}$ appearing in \eqref{eq:1} can be represented as
\begin{equation}
  \label{eq:13}
  \M{U} = \Mt{U} \Mt{C},
 \end{equation}
 where $\Mt{U} \in \Mat{p}{r}$ and $\Mt{C} \in \Mat{r}{r}$, and
\begin{equation}
  \label{eq:14}
  \M{V} = \Mt{V} \Mt{D}, 
 \end{equation}
 where $\Mt{V} \in \Mat{q}{r}$ and $\Mt{D} \in \Mat{r}{r}$.  Then, as it turns out, the matrices $\M{U}'$ and $\M{V}'$ given in \eqref{eq:4} and \eqref{eq:5} can also be represented in a similar fashion as
\begin{equation}
  \label{eq:15}
  \M{U}' = \Mt{U}' \Mt{C}', 
\end{equation}
where $\Mt{U}' \in \Mat{p}{k}$ and $\Mt{C}' \in \Mat{k}{k}$, and
\begin{equation}
  \label{eq:16}
  \M{V}' = \Mt{V}' \Mt{D}', 
\end{equation}
where $\Mt{V}' \in \Mat{q}{k}$ and $\Mt{D}' \in \Mat{k}{k}$.  The explicit formulae for $\Mt{U}'$, $\Mt{C}'$, $\Mt{V}'$, and $\Mt{D}'$, which we shall derive next, are intended to be used iteratively to obtain an expression for the thin SVD of a matrix formed by adding in succession a number of columns to a given matrix. In this target scenario, the matrices $\M{U}$ and $\M{V}$ corresponding to the initial matrices are represented as $\M{U} = \Mt{U} \Mt{C}$ and $\M{V} = \Mt{V} \Mt{D}$, where $\Mt{U} = \M{U}$, $\Mt{C} = \M{I}_r$, $\Mt{V} = \M{V}$, and $\Mt{D} = \M{I}_r$.

\subsubsection{A formula for $\M{U}'$}

We consider two cases.

\begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
\item Suppose that $\ve{p} \neq \0$ (or, equivalently, that $k= r + 1$). Then, in view \eqref{eq:4} and \eqref{eq:13},
\begin{displaymath}
  \M{U}' = [\M{U}, \ve{p}] \M{C} = [\Mt{U} \Mt{C}, \ve{p}] \M{C}
  =
  [\Mt{U}, \ve{p}]
  \begin{bmatrix}
    \Mt{C} & \0
    \\
    \0^\T & 1
  \end{bmatrix}
  \M{C}.
\end{displaymath}
We now obtain the desired representation 
\begin{math}
  \M{U}' = \Mt{U}' \Mt{C}'
\end{math}
by letting
\begin{displaymath}
  \Mt{U}' = [\Mt{U}, \ve{p}]
  \quad
  \text{and}
  \quad
  \Mt{C}' =
  \begin{bmatrix}
    \Mt{C} & \0
    \\
    \0^\T & 1
  \end{bmatrix}
  \M{C}.
\end{displaymath}
\item Suppose that $\ve{p} = \0$ (or, equivalently, that $k = r$). Then, in view of \eqref{eq:4}, \eqref{eq:11}, and the observation that now $\M{C}^{1:r,:} = \M{C}^{1:r, 1:r}$,
\begin{displaymath}
  \M{U}' = [\M{U}, \ve{p}] \M{C}
  = [\M{U}, \0]
  \begin{bmatrix}
    \M{C}^{1:r, 1:r}
    \\
    \0^\T
  \end{bmatrix}
  = \M{U} \M{C}^{1:r, 1:r}
  = \Mt{U} \Mt{C}  \M{C}^{1:r, 1:r}.
\end{displaymath}
This leads to the representation
\begin{math}
  \M{U}' = \Mt{U}' \Mt{C}'
\end{math}
with
\begin{displaymath}
  \Mt{U}' =  \Mt{U}
  \quad
  \text{and}
  \quad
  \Mt{C}' = \Mt{C}  \M{C}^{1:r, 1:r}.
\end{displaymath}
\end{enumerate}

\subsubsection{A formula for $\M{V}'$}

We consider two cases.

\begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
\item Suppose that $\ve{p} \neq \0$ (or, equivalently, that $k= r + 1$).  Then
  \begin{displaymath}
    \M{V}' =
    \begin{bmatrix}
      \M{V} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \M{D}
    =
    \begin{bmatrix}
      \Mt{V} \Mt{D} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \M{D}
    =
    \begin{bmatrix}
      \Mt{V} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \begin{bmatrix}
      \Mt{D} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \M{D}.
  \end{displaymath}
  Accordingly, we obtain the representation
  \begin{math}
    \M{V}' = \Mt{V}' \Mt{D}'
  \end{math}
  by letting
  \begin{displaymath}
    \Mt{V}' =
    \begin{bmatrix}
      \Mt{V} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \quad
    \text{and}
    \quad
    \Mt{D}' =
    \begin{bmatrix}
      \Mt{D} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \M{D}.
  \end{displaymath}
\item Suppose that $\ve{p} = \0$ (or, equivalently, that $k = r$).  Then $\M{D}$
  is a $(r+1) \times r$ matrix and we have
  \begin{displaymath}
    \M{D}
    =
    \begin{bmatrix}
      \M{D}^{1:r,1:r}
      \\
      \M{D}^{r+1,1:r}
    \end{bmatrix}.
  \end{displaymath}
  Here $\M{D}^{r+1,1:r}$ stands for the last row of $\M{D}$.  In view of \eqref{eq:5},
  \begin{displaymath}
    \M{V}' =
    \begin{bmatrix}
      \M{V} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \M{D}
    =
    \begin{bmatrix}
      \M{V} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \begin{bmatrix}
      \M{D}^{1:r,1:r}
      \\
      \M{D}^{r+1,1:r}
    \end{bmatrix}
    =
    \begin{bmatrix}
      \M{V} \M{D}^{1:r,1:r}
      \\
      \M{D}^{r+1,1:r}
    \end{bmatrix}.
  \end{displaymath}
  At this stage we make an additional assumption that $\| \M{D}^{r+1,1:r} \| \neq 1$. Then $\M{D}^{1:r,1:r}$ is invertible, with inverse
  \begin{displaymath}
    (\M{D}^{1:r,1:r})^{-1}
    = \left(
      \M{I}_r
      + \frac{1}{1 - \| \M{D}^{r+1,1:r} \| ^2}
      \left(\M{D}^{r+1,1:r}\right)^\T
      \M{D}^{r+1,1:r}
    \right)
    \left(\M{D}^{1:r,1:r}\right)^\T.
  \end{displaymath}
  This can easily be inferred from the fact that
  \begin{math}
    \M{D}^\T \M{D} = \M{I}_r,
  \end{math}
  or, what is the same, that
  \begin{displaymath}
    \left(\M{D}^{1:r,1:r}\right)^\T\M{D}^{1:r,1:r}
    +
    \left(\M{D}^{r+1,1:r}\right)^\T
    \M{D}^{r+1,1:r}
    =
    \M{I}_r.
  \end{displaymath}
  The $r \times r$ matrix $\Mt{D}$ is invertible, for otherwise, in view of \eqref{eq:14}, $\M{V}$ would have rank smaller than $r$. Therefore
  \begin{math}
    \Mt{D} \M{D}^{1:r,1:r}
  \end{math}
  is invertible, with inverse
  \begin{displaymath}
    \left(\Mt{D} \M{D}^{1:r,1:r}\right)^{-1}
    =
    \left(\M{D}^{1:r,1:r}\right)^{-1}
    \Mt{D}^{-1}.
  \end{displaymath}
  Now, using \eqref{eq:14}, we have
  \begin{align*}
    % \M{V}'
    % & =
        \begin{bmatrix}
          \M{V} \M{D}^{1:r,1:r}
          \\
          \M{D}^{r+1,1:r}
        \end{bmatrix}
      & =
        \begin{bmatrix}
          \Mt{V} \Mt{D} \M{D}^{1:r,1:r}
          \\
          \M{D}^{r+1,1:r}
        \end{bmatrix}
    =
    \begin{bmatrix}
      \Mt{V} \Mt{D} \M{D}^{1:r, 1:r}
      \\
      \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^{-1} \Mt{D} \M{D}^{1:r, 1:r}
    \end{bmatrix}
    \\
      & =
        \begin{bmatrix}
          \Mt{V}
          \\
          \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^{-1}
        \end{bmatrix}
    \Mt{D} \M{D}^{1:r, 1:r}.
  \end{align*}
  This leads to the representation
  \begin{math}
    \M{V}' = \Mt{V}' \Mt{D}'
  \end{math}
  with
  \begin{displaymath}
    \Mt{V}' =
    \begin{bmatrix}
      \Mt{V}
      \\
      \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^{-1}
    \end{bmatrix}
    \quad
    \text{and}
    \quad
    \Mt{D}' =
    \Mt{D} \M{D}^{1:r, 1:r}.
  \end{displaymath}
  For the sake of agreement with the formulation introduced in~\cite{brand06:_fast}, we let
   \begin{displaymath}
    \Mt{V}' =
    \begin{bmatrix}
      \Mt{V}
      \\
      \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^+
    \end{bmatrix}.
  \end{displaymath}
  Here, for a given matrix $\M{A}$, we use $\M{A}^+$ to denote the pseudo-inverse (or Moore--Penrose inverse) of $\M{A}$.
\end{enumerate}

\subsection{Iterative updating}

We are now in a position to present an algorithm for calculating the SVD of a matrix formed by adding in succession a number of columns to a given matrix. This algorithm was first developed and presented in~\cite{brand06:_fast}.

\begin{algorithm}[\textnormal{\textsc{ThinSVD}(\unboldmath$[\M{X}, \ve{c}_1, \dots, \ve{c}_K]$)}]
% \begin{algorithm}[\textnormal{\textsc{ThinSVD}(\mbox{$[\M{X}, \ve{c}_1, \dots, \ve{c}_K]$})}]
  \rule{\textwidth}{0.4pt}
  \begin{algorithmic}[1]
    \Require $\M{X} \in \Mat{p}{q}$, $\ve{c}_1 \in \Mat{p}{1}$, \dots, $\ve{c}_K \in \Mat{p}{1}$
    \State $(\M{U}, \M{S}, \M{V}) \gets \textsc{ThinSVD}(\M{X})$
    \State $r \gets \textsc{NumberOfColumns/Rows}(\M{S})$
    \State $\Mt{U} \gets \M{U}$
    \State $\Mt{C} \gets \M{I}_r$
    \State $\Mt{V} \gets \M{V}$
    \State $\Mt{D} \gets \M{I}_r$
    \For{$k \gets 1 \ldots K$}
    \If {$\ve{c}_k - \M{U}\M{U}^\T \ve{c}_k \neq \0$ }
    \State $\ve{p} \gets \frac{\ve{c}_k - \M{U}\M{U}^\T \ve{c}_k}{\| \ve{c}_k - \M{U}\M{U}^\T \ve{c}_k\|} $
    \Else
    \State $\ve{p} \gets \0$
    \EndIf
    \State  
    \begin{math}
      \M{K}
      \gets
      \begin{bmatrix}
        \M{S} & \M{U}^\T \ve{c}_k \\
        \0^\T & \| \ve{c}_k - \M{U} \M{U}^\T \ve{c}_k \|
      \end{bmatrix}
    \end{math}
    \State
    $(\M{C}, \M{S}', \M{D}) \gets \textsc{ThinSVD}(\M{K})$
    \If {$\ve{p} \neq \0$}
    \State
    \begin{math}
      \Mt{U} \gets [\Mt{U}, \ve{p}]
    \end{math}
    \State
    \begin{math}
      \Mt{C}
      \gets
      \begin{bmatrix}
        \Mt{C} & \0
        \\
        \0^\T & 1
      \end{bmatrix}
      \M{C}
    \end{math}
    \State
    \begin{math}
      \Mt{V}
      \gets
      \begin{bmatrix}
        \Mt{V} & \0
        \\
        \0^\T & 1
      \end{bmatrix} 
    \end{math}
    \State
    \begin{math}
      \Mt{D}
      \gets
      \begin{bmatrix}
        \Mt{D} & \0
        \\
        \0^\T & 1
      \end{bmatrix}
      \M{D}
    \end{math}
    \Else
    \State
    \begin{math}
      \Mt{C}
      \gets
      \Mt{C}  \M{C}^{1:r, 1:r}
    \end{math}
    \State
    \begin{math}
      \Mt{V}
      \gets
      \begin{bmatrix}
        \Mt{V}
        \\
        \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^+
      \end{bmatrix}
    \end{math}
    \State
    \begin{math}
      \Mt{D}'
      \gets
      \Mt{D} \M{D}^{1:r, 1:r}.
    \end{math}
    \EndIf
    \State
    \begin{math}
      \M{U} \gets \Mt{U} \Mt{C}
    \end{math}
    \State
    \begin{math}
      \M{S} \gets \M{S}'
    \end{math}
    \State
    \begin{math}
      r \gets \textsc{NumberOfColumns/Rows}(\M{S})
    \end{math}
    \EndFor
    \State
    \begin{math}
      \M{V} \gets \Mt{V} \Mt{D}
    \end{math}
    \State
    \Return $(\M{U}, \M{S}, \M{V})$
  \end{algorithmic}
\end{algorithm}

% \subsection{A representation}

% \begin{displaymath}
%   \M{U} = \Mt{U} \Mt{C}
% \end{displaymath}

% Suppose that $\ve{p} \neq \0$. Note that
% \begin{displaymath}
%   [\Mt{U} \Mt{C}, \ve{p}]
%   =
%   [\Mt{U}, \ve{p}]
%   \begin{bmatrix}
%     \Mt{C} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}.
% \end{displaymath}
% Therefore
% \begin{displaymath}
%   \M{U}' = [\M{U}, \ve{p}] \M{C} = [\Mt{U} \Mt{C}, \ve{p}] \M{C}
%   =
%   [\Mt{U}, \ve{p}]
%   \begin{bmatrix}
%     \Mt{C} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{C}
% \end{displaymath}
% Let
% \begin{displaymath}
%   \Mt{U}' = [\Mt{U}, \ve{p}]
%   \quad
%   \text{and}
%   \quad
%   \Mt{C}' =
%   \begin{bmatrix}
%     \Mt{C} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{C}.
% \end{displaymath}
% Then
% \begin{displaymath}
%   \M{U}' = \Mt{U}' \Mt{C}'
% \end{displaymath}

% Suppose that $\ve{p} = \0$. Then
% \begin{displaymath}
%   \M{U}' = [\M{U}, \ve{p}] \M{C}
%   = [\M{U}, \0]
%   \begin{bmatrix}
%     \M{C}^{1:r, 1:r}
%     \\
%     \0^\T
%   \end{bmatrix}
%   = \M{U} \M{C}^{1:r, 1:r}
%   = \Mt{U} \Mt{C}  \M{C}^{1:r, 1:r}
% \end{displaymath}
% Let
% \begin{displaymath}
%   \Mt{U}' =  \Mt{U}
%   \quad
%   \text{and}
%   \quad
%   \Mt{C}' = \Mt{C}  \M{C}^{1:r, 1:r}
% \end{displaymath}
% Then
% \begin{displaymath}
%   \M{U}' = \Mt{U}' \Mt{C}'
% \end{displaymath}

% \subsection{Another representation}

% \begin{displaymath}
%   \M{V} = \Mt{V} \Mt{D}
% \end{displaymath}

% Suppose that $\ve{p} \neq \0$. Then
% \begin{displaymath}
%   \M{V}' =
%   \begin{bmatrix}
%     \M{V} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}
%   =
%   \begin{bmatrix}
%     \Mt{V} \Mt{D} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}
%   =
%   \begin{bmatrix}
%     \Mt{V} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \begin{bmatrix}
%     \Mt{D} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}
% \end{displaymath}
% Let
% \begin{displaymath}
%   \Mt{V}' =
%   \begin{bmatrix}
%     \Mt{V} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \quad
%   \text{and}
%   \quad
%   \Mt{D}' =
%   \begin{bmatrix}
%     \Mt{D} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}.
% \end{displaymath}
% Then
% \begin{displaymath}
%   \M{V}' = \Mt{V}' \Mt{D}'.
% \end{displaymath}

% Suppose that $\ve{p} = \0$. Then
% \begin{align*}
%   \M{V}'
%   & =
%     \begin{bmatrix}
%       \M{V} \M{D}^{1:r,1:r}
%       \\
%       \M{D}^{r+1,1:r}
%     \end{bmatrix}
%   =
%   \begin{bmatrix}
%     \Mt{V} \Mt{D} \M{D}^{1:r,1:r}
%     \\
%     \M{D}^{r+1,1:r}
%   \end{bmatrix}
%   =
%   \begin{bmatrix}
%     \Mt{V} \Mt{D} \M{D}^{1:r, 1:r}
%     \\
%     \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^+ \Mt{D} \M{D}^{1:r, 1:r}
%   \end{bmatrix}
%   \\
%   & =
%     \begin{bmatrix}
%       \Mt{V}
%       \\
%       \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^+
%     \end{bmatrix}
%   \Mt{D} \M{D}^{1:r, 1:r}.
% \end{align*}
% Let
% \begin{displaymath}
%   \Mt{V}' =
%   \begin{bmatrix}
%     \Mt{V}
%     \\
%     \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^+
%   \end{bmatrix}
%   \quad
%   \text{and}
%   \quad
%   \Mt{D}' =
%   \Mt{D} \M{D}^{1:r, 1:r}.
% \end{displaymath}
% Then
% \begin{displaymath}
%   \M{V}' =  \Mt{V}' \Mt{D}'.
% \end{displaymath}
 

% \subsection{The Iterative Process}

% Let
% \begin{displaymath}
%   \M{X}_0 = \M{U}_0 \M{S}_0 \M{V}_0^\T.
% \end{displaymath}
% Then
% \begin{displaymath}
%   \M{X}_1
%   = [\M{U}_0, \ve{p}_1] \M{K}_1
%   \begin{bmatrix}
%     \M{V}_0 & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}^\T = [\M{U}_0, \ve{p}_1] \M{C}_1 \M{S}_1 \M{D}_1^\T
%   \begin{bmatrix}
%     \M{V}_0 & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}^\T.
% \end{displaymath}
% With
% \begin{displaymath}
%   \M{U}_1 =  [\M{U}_0, \ve{p}_1] \M{C}_1
%   \quad
%   \text{and}
%   \quad
%   \M{V}_1 =
%   \begin{bmatrix}
%     \M{V}_0 & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_1,
% \end{displaymath}
% we have
% \begin{displaymath}
%   \M{X}_1 =   \M{U}_1 \M{S}_1 \M{V}_1^\T.
% \end{displaymath}
% Generally, for any $k \in \N$,
% \begin{displaymath}
%   \M{X}_k
%   =
%   [\M{U}_{k-1}, \ve{p}_k] \M{K}_k
%   \begin{bmatrix}
%     \M{V}_{k-1} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}^\T = [\M{U}_{k-1}, \ve{p}_k] \M{C}_k \M{S}_k \M{D}_k^\T
%   \begin{bmatrix}
%     \M{V}_{k-1} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}^\T,
% \end{displaymath}
% and with
% \begin{displaymath}
%   \M{U}_k =  [\M{U}_{k-1}, \ve{p}_k] \M{C}_k
%   \quad
%   \text{and}
%   \quad
%   \M{V}_k
%   =
%   \begin{bmatrix}
%     \M{V}_{k-1} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_k,
% \end{displaymath}
% we have
% \begin{displaymath}
%   \M{X}_k =  \M{U}_k \M{S}_k \M{V}_k^\T.
% \end{displaymath}

% \subsection{Updating the $\M{U}$ matrices}

% Note that, for each $k \in \N$,
% \begin{align*}
%   \M{U}_{k+1}
%   &
%     = [\M{U}_k, \ve{p}_{k+1}] \M{C}_{k+1}
%     =  [[\M{U}_{k-1}, \ve{p}_k] \M{C}_k, \ve{p}_{k+1}] \M{C}_{k+1}
%   \\
%   &
%     =
%     \left[
%     [\M{U}_{k-1}, \ve{p}_k], \ve{p}_{k+1}
%     \right]
%     \begin{bmatrix}
%       \M{C}_k & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix}
%               \M{C}_{k+1}.
% \end{align*}
% This equality inspires us to introduce, inductively, the following matrices:
% \begin{displaymath}
%   \Mt{U}_1 = [\M{U}_0, \ve{p}_1]
%   \quad
%   \text{and}
%   \quad
%   \Mt{C}_1 = \M{C}_1,
% \end{displaymath}
% and, for each positive integer $k$ greater than 1,
% \begin{displaymath}
%   \Mt{U}_k
%   =
%   [ \Mt{U}_{k-1},  \ve{p}_k]
%   \
%   (
%   = 
%   [\M{U}_0, \ve{p}_1, \dots, \ve{p}_k]
%   )
%   \quad
%   \text{and}
%   \quad
%   \Mt{C}_k
%   =
%   \begin{bmatrix}
%     \Mt{C}_{k-1} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{C}_k.
% \end{displaymath}
% We claim that
% \begin{equation}
%   \label{eq:17}
%   \M{U}_k = \Mt{U}_k \Mt{C}_k
% \end{equation}
% for each $k \in \N$.  Indeed, the claim holds vacuously for $k = 1$. Assume that the claim holds for $k$. Then, since
% \begin{displaymath}
%   \Mt{U}_{k+1} = [\Mt{U}_k, \ve{p}_{k+1}]
%   \quad
%   \text{and}
%   \quad
%   \Mt{C}_{k+1}
%   =
%   \begin{bmatrix}
%     \Mt{C}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{C}_{k+1},
% \end{displaymath}
% we have
% \begin{displaymath}
%   \Mt{U}_{k+1} \Mt{C}_{k+1}
%   =
%   [\Mt{U}_k, \ve{p}_{k+1}]
%   \begin{bmatrix}
%     \Mt{C}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{C}_{k+1}
%   \\
%   =
%   [\Mt{U}_k  \Mt{C}_k, \ve{p}_{k+1}] \M{C}_{k+1}.
% \end{displaymath}
% By the inductive hypothesis, expressed as \eqref{eq:17},
% \begin{displaymath}
%   [\Mt{U}_k  \Mt{C}_k, \ve{p}_{k+1}] \M{C}_{k+1}
%   =
%   [\M{U}_k, \ve{p}_{k+1}] \M{C}_{k+1}.
% \end{displaymath}
% But
% \begin{displaymath}
%   [\M{U}_k, \ve{p}_{k+1}] \M{C}_{k+1}
%   =
%   \M{U}_{k+1}
% \end{displaymath}
% by definition.  Thus
% \begin{displaymath}
%   \M{U}_{k+1} = \Mt{U}_{k+1} \Mt{C}_{k+1},
% \end{displaymath}
% and so the claim holds for $k+1$.

% \subsection{Updating the $\M{V}$ matrices}

% Note that, for each $k \in \N$,
% \begin{align*}
%   \M{V}_{k+1}
%   &
%     =
%     \begin{bmatrix}
%       \M{V}_k & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix}
%              \M{D}_{k+1}
%              =
%              \begin{bmatrix}
%                \begin{bmatrix}
%                  \M{V}_{k-1} & \0
%                  \\
%                  \0^\T & 1
%                \end{bmatrix}
%                \M{D}_k & \0
%                \\
%                \0^\T & 1
%              \end{bmatrix}
%                       \M{D}_{k+1}
%   \\
%   & =
%     \begin{bmatrix}
%       \begin{bmatrix}
%         \M{V}_{k-1} & \0
%         \\
%         \0^\T & 1
%       \end{bmatrix}
%       & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix}
%              \begin{bmatrix}
%                \M{D}_k & \0
%                \\
%                \0^\T & 1
%              \end{bmatrix}
%                        \M{D}_{k+1}.
% \end{align*}
% This equality suggests that we introduce, inductively, the following matrices:
% \begin{displaymath}
%   \Mt{V}_1
%   =
%   \begin{bmatrix}
%     \M{V}_0 & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \quad
%   \text{and}
%   \quad
%   \Mt{D}_1 = \M{D}_1,
% \end{displaymath}
% and, for each positive integer $k$ greater than 1,
% \begin{displaymath}
%   \Mt{V}_k
%   =
%   \begin{bmatrix}
%     \Mt{V}_{k-1} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \
%   \left(=
%     \begin{bmatrix}
%       \M{V}_0 & \0 & \0 & \0 & \cdots & \0
%       \\
%       \0^\T & 1 & 0 & 0 & \cdots & 0
%       \\
%       \0^\T & 0 & 1 & 0 & \cdots & 0
%       \\
%       \vdots & \vdots & \vdots & \ddots & & \vdots
%       \\
%       \vdots & \vdots & \vdots & & \ddots & \vdots
%       \\
%       \0^\T & \undermat{\text{$k$ columns}}{0 & 0 & 0 & \cdots & 1}
%     \end{bmatrix}
%   \right)
% \end{displaymath}
% and
% \begin{displaymath}
%   \Mt{D}_k
%   =
%   \begin{bmatrix}
%     \Mt{D}_{k-1} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_k.
% \end{displaymath}
% We claim that
% \begin{equation}
%   \label{eq:18}
%   \M{V}_k = \Mt{V}_k \Mt{D}_k
% \end{equation}
% for each $k \in \N$.  Indeed, the claim holds vacuously for $k = 1$. Assume that the claim holds for $k$. Then, since
% \begin{displaymath}
%   \Mt{V}_{k+1}
%   =
%   \begin{bmatrix}
%     \Mt{V}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \quad
%   \text{and}
%   \quad
%   \Mt{D}_{k+1}
%   =
%   \begin{bmatrix}
%     \Mt{D}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_{k+1},
% \end{displaymath}
% we have
% \begin{displaymath}
%   \Mt{V}_{k+1} \Mt{D}_{k+1}
%   =
%   \begin{bmatrix}
%     \Mt{V}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \begin{bmatrix}
%     \Mt{D}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_{k+1}
%   \\
%   =
%   \begin{bmatrix}
%     \Mt{V}_k \Mt{D}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_{k+1}.
% \end{displaymath}
% By the inductive hypothesis, expressed as \eqref{eq:18},
% \begin{displaymath}
%   \begin{bmatrix}
%     \Mt{V}_k \Mt{D}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_{k+1}
%   =
%   \begin{bmatrix}
%     \M{V}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_{k+1}.
% \end{displaymath}
% But
% \begin{displaymath}
%   \begin{bmatrix}
%     \M{V}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}_{k+1}
%   =
%   \M{V}_{k+1}
% \end{displaymath}
% by definition.  Thus
% \begin{displaymath}
%   \M{V}_{k+1} = \Mt{V}_{k+1} \Mt{D}_{k+1},
% \end{displaymath}
% and so the claim holds for $k+1$.

% \subsection{Updating when the rank does not increase}

% \subsubsection{Description}

% For each $n \in \N$, let $r_n$ denote the rank of $\M{K}_n$. Suppose that $k \in \N$ is such that $r_{k+1} = r_k$. Then the unreduced SVD of $\M{K}_{k+1}$ takes the form
% \begin{displaymath}
%   \M{K}_{k+1} = \M{C}_{k+1} \M{S}_{k+1} \M{D}^\T_{k+1},
% \end{displaymath}
% where
% \begin{align*}
%   \M{C}_{k+1}
%   & =
%     \begin{bmatrix}
%       \M{C}^{1:r_k, 1:r_k}_{k+1} & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix},
%   \\[2ex]
%   \M{S}_{k+1} & = \diag(s_{1,k+1}, \dots, s_{r_k,k+1}, 0),
%   \\[2ex]
%   \M{D}_{k+1} & =
%                 \begin{bmatrix}
%                   \M{D}^{1:r_k, 1:r_k}_{k+1} & \M{D}^{1:r_k, r_k+1}_{k+1}
%                   \\
%                   \M{D}^{r_k+1, 1:r_k}_{k+1} & \M{D}^{r_k+1, r_k+1}_{k+1}
%                 \end{bmatrix}.
% \end{align*}
% The matrix $\M{K}_{k+1}$ admits also an SVD in reduced form, namely
% \begin{displaymath}
%   \M{K}_{k+1} = \Mr{C}_{k+1} \Mr{S}_{k+1} (\Mr{D}_{k+1})^\T,
% \end{displaymath}
% where
% \begin{align*}
%   \Mr{C}_{k+1}
%   & =
%     \begin{bmatrix}
%       \M{C}^{1:r_k, 1:r_k}_{k+1}
%       \\
%       \0^\T
%     \end{bmatrix},
%   \\[2ex]
%   \Mr{S}_{k+1} & = \diag(s_{1,k+1}, \dots, s_{r_k,k+1}),
%   \\[2ex]
%   \Mr{D}_{k+1} & =
%                  \begin{bmatrix}
%                    \M{D}^{1:r_k, 1:r_k}_{k+1}
%                    \\
%                    \M{D}^{r_k+1, 1:r_k}_{k+1}
%                  \end{bmatrix}.
% \end{align*}
% Exploiting the latter representation, one can show that if we adopt
% \begin{displaymath}
%   \Mt{U}_{k+1} =  \Mt{U}_k
%   \quad
%   \text{and}
%   \quad
%   \Mt{C}_{k+1} = \Mt{C}_k \M{C}^{1:r_k, 1:r_k}_{k+1}
% \end{displaymath}
% for an update rule for the matrices $\Mt{U}_k$ and $\Mt{C}_k$, and if we adopt
% \begin{displaymath}
%   \Mt{V}_{k+1}
%   =
%   \begin{bmatrix}
%     \Mt{V}_k
%     \\
%     \M{D}^{r_k+1, 1:r_k}_{k+1} \left(\Mt{D}_k \M{D}^{1:r_k,1:r_k}_{k+1}\right)^+
%   \end{bmatrix}
%   \quad
%   \text{and}
%   \quad
%   \Mt{D}_{k+1}
%   =
%   \Mt{D}_k
%   \M{D}^{1:r_k,1:r_k}_{k+1}
% \end{displaymath}
% for an update rule for the matrices $\Mt{V}_k$ and $\Mt{D}_k$, then the SVD of $\M{X}_{k+1}$ is given by
% \begin{displaymath}
%   \M{X}_{k+1} = \Mt{C}_{k+1} \Mr{S}_{k+1} \Mt{D}^\T_{k+1}.
% \end{displaymath}

% \subsubsection{Explanation}

% \begin{align*}
%   \begin{bmatrix}
%     \Mt{V}_k \Mt{D}_k & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%             \M{D}_{k+1}
%           & =
%             \begin{bmatrix}
%               \Mt{V}_k \Mt{D}_k & \0
%               \\
%               \0^\T & 1
%             \end{bmatrix}
%                       \begin{bmatrix}
%                         \M{D}^{1:r_k, 1:r_k}_{k+1} & \M{D}^{1:r_k, r_k+1}_{k+1}
%                         \\
%                         \M{D}^{r_k+1, 1:r_k}_{k+1} & \M{D}^{r_k+1, r_k+1}_{k+1}
%                       \end{bmatrix}
%   \\
%                       & =
%                         \begin{bmatrix}
%                           \Mt{V}_k \Mt{D}_k \M{D}^{1:r_k, 1:r_k}_{k+1} & \Mt{V}_k \Mt{D}_k \M{D}^{r_k+1, 1:r_k}_{k+1}
%                           \\
%                           \M{D}^{r_k+1, 1:r_k}_{k+1} & \M{D}^{r_k+1, r_k+1}_{k+1}
%                         \end{bmatrix}
% \end{align*}
% Deleting the last column, we obtain the matrix
% \begin{displaymath}
%   \begin{bmatrix}
%     \Mt{V}_k \Mt{D}_k \M{D}^{1:r_k, 1:r_k}_{k+1}
%     \\
%     \M{D}^{r_k+1, 1:r_k}_{k+1}
%   \end{bmatrix},
% \end{displaymath}
% which we represent as
% \begin{align*}
%   \begin{bmatrix}
%     \Mt{V}_k \Mt{D}_k \M{D}^{1:r_k, 1:r_k}_{k+1}
%     \\
%     \M{D}^{r_k+1, 1:r_k}_{k+1}
%   \end{bmatrix}
%   & =
%     \begin{bmatrix}
%       \Mt{V}_k \Mt{D}_k \M{D}^{1:r_k, 1:r_k}_{k+1}
%       \\
%       \M{D}^{r_k+1, 1:r_k}_{k+1}
%     \end{bmatrix}
%   \\
%   & =
%     \begin{bmatrix}
%       \Mt{V}_k \Mt{D}_k \M{D}^{1:r_k, 1:r_k}_{k+1}
%       \\
%       \M{D}^{r_k+1, 1:r_k}_{k+1} \left(\Mt{D}_k \M{D}^{1:r_k,1:r_k}_{k+1}\right)^+ \Mt{D}_k \M{D}^{1:r_k, 1:r_k}_{k+1}
%     \end{bmatrix}
%   \\
%   & =
%     \underbrace{%
%     \begin{bmatrix}
%       \Mt{V}_k
%       \\
%       \M{D}^{r_k+1, 1:r_k}_{k+1} \left(\Mt{D}_k \M{D}^{1:r_k,1:r_k}_{k+1}\right)^+
%     \end{bmatrix}
%   }_{\Mt{V}_{k+1}}
%   \underbrace{%
%   \Mt{D}_k \M{D}^{1:r_k, 1:r_k}_{k+1}
%   }_{\Mt{D}_{k+1}}.
% \end{align*}

\section{Zeroing and downdating}

\subsection{Preliminaries}

For two integers $a$ and $b$ with $a \leq b$, let $\Brackets{a,b}$ denote the interval of all integers between $a$ and $b$, with $a$ and $b$ included. When $a$ is an integer no smaller than $1$, $\Brackets{1,a}$ is abbreviated to $\Brackets{a}$. The symbol $\SemiOpenBrackets{a, \infty}$ will denote the interval of all integers no smaller than $a$.

Given $N \in \N$ and $n \in \Brackets{N}$, let $\eve{n}{N} = [0, \dots, 0, 1, 0, \dots, 0]^\T$ be the length-$N$ vector with $1$ in the $n$-th position and $0$ in all others.

Given $p \in \SemiOpenBrackets{2,\infty}$ and $i \in \Brackets{p}$, let $\sel{i}{p}$ denote the $(p-1) \times p$ \emph{selection matrix} given by
\begin{align*}
  \sel{i}{p}
  & =
    [\eve{1}{p-1}, \dots, \eve{i-1}{p-1},
    \0_{p-1}, \eve{i}{p-1}, \dots, \eve{p-1}{p-1}]
  \\
  & =
    \eve{1}{p-1}\big(\eve{1}{p}\big)^\T
    +
    \dots
    +
    \eve{i-1}{p-1}\big(\eve{i-1}{p}\big)^\T
  \\
  & \quad
    +
    \eve{i}{p-1}\big(\eve{i+1}{p}\big)^\T
    +
    \dots
    +
    \eve{p-1}{p-1}\big(\eve{p}{p}\big)^\T.
\end{align*}
To get an idea of the shape of selection matrices, consider the following example:
\begin{displaymath}
  \sel{3}{5}
  =
  \begin{bmatrix}
    1 & 0 & 0 & 0 & 0
    \\
    0 & 1 & 0 & 0 & 0
    \\
    0 & 0 & 0 & 1 & 0
    \\
    0 & 0 & 0 & 0 & 1
  \end{bmatrix}.
\end{displaymath}
The rationale for the term ``selection matrix'' is that selection matrices can be used to select a subset of columns or rows of a matrix---see \eqref{eq:20} below.

Let $\M{A} \in \Mat{p}{q}$, and let $i \in \Brackets{p}$ and $j \in \Brackets{q}$. Let $\zerrow{A}{i}$ denote the $p \times q$ matrix resulting from replacing the $i$th row of $\M{A}$ by a zero row vector, and let $\zercol{A}{j}$ denote the $p \times q$ matrix resulting from by replacing the $j$th column of $\M{A}$ by a zero column vector.  We have
\begin{equation}
  \label{eq:19}
  \zerrow{A}{i} = \M{A} - \eve{i}{p} (\eve{i}{p})^\T \M{A} 
  \quad
  \text{and}
  \quad
  \zercol{A}{j} = \M{A} - \M{A} \eve{j}{q} (\eve{j}{q})^\T.
\end{equation}

Supposing that $p \geq 2$, let $\delrow{A}{i}$ denote the $(p - 1) \times q$ matrix resulting from deleting the $i$th row from $\M{A}$, and supposing that $q \geq 2$, let $\delcol{A}{j}$ denote the $p \times (q - 1)$ matrix resulting from deleting the $j$th column from $\M{A}$.  It turns out that
\begin{equation}
  \label{eq:20}
  \delrow{A}{i} = \sel{i}{p} \M{A}
  \quad
  \text{and}
  \quad
  \delcol{A}{j} = \M{A} \sel{j}{q}^\T.
\end{equation}

\subsection{The method}

Let $\M{X} \in \Mat{p}{q}$ be a matrix of rank $r \geq 1$, and let the thin SVD of $\M{X}$ be given as in \eqref{eq:1}. Let $j \in \Brackets{q}$. We are interested in finding formulas for the thin SVD of $\zercol{X}{j}$ and $\delcol{X}{j}$.

The key to obtaining the relevant expressions is the representation
\begin{equation}
  \label{eq:21}
  \zercol{X}{j}
  =
  [\M{U}, \0] \M{K} [\M{V}, \ve{q}]^\T,
\end{equation}
where $\ve{q} \in \R^q$ is given by
\begin{displaymath}
  \ve{q} =
  \begin{cases}
    \frac{\eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}}{\| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|} & \text{if $\eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q} \neq \0$,}
    \\
    \0 & \text{otherwise,}
  \end{cases}
\end{displaymath}
and $\M{K} \in \Mat{(r+1)}{(r+1)}$ is given by
\begin{displaymath}
  \M{K}
  =
  \begin{bmatrix}
    \M{S} & \0
    \\
    \0^\T & 0
  \end{bmatrix}
  \left(
    \M{I}_{r+1}
    -
    \begin{bmatrix}
      \M{V}^\T \eve{j}{q}
      \\
      0
    \end{bmatrix}
    \begin{bmatrix}
      \M{V}^\T \eve{j}{q}
      \\[1.5ex]
      \sqrt{1 - \| \M{V}^\T \eve{j}{q} \|^2}
    \end{bmatrix}^\T \right).
\end{displaymath}
We immediately point out that the occurrence of $\sqrt{1 - \| \M{V}^\T \eve{j}{q} \|^2}$ in the expression for $\M{K}$ reflects the fact that
\begin{equation}
  \label{eq:22}
  \sqrt{1 - \| \M{V}^\T \eve{j}{q} \|^2}
  =
  \| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|.
\end{equation}
To prove the above equality, note that
\begin{align*}
  \| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|^2
  & =
    \| \eve{j}{q} \|^2 +  \| \M{V} \M{V}^\T \eve{j}{q}\|^2
    - 2 \big(\eve{j}{q}\big)^\T \M{V} \M{V}^\T \eve{j}{q}
  \\
  & =
    1 + \| \M{V} \M{V}^\T \eve{j}{q}\|^2 - 2 \| \M{V}^\T \eve{j}{q} \|^2.
\end{align*}
Now
\begin{displaymath}
  \| \M{V} \M{V}^\T \eve{j}{q}\|^2
  =
  \big(\eve{j}{q}\big)^\T \M{V} \M{V}^\T \M{V} \M{V}^\T \eve{j}{q}
  =
  \big(\eve{j}{q}\big)^\T \M{V} \M{V}^\T \eve{j}{q}
  =
  \| \M{V}^\T \eve{j}{q} \|^2,
\end{displaymath}
and hence
\begin{displaymath}
  \| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|^2
  =
  1 + \| \M{V}^\T \eve{j}{q} \|^2 - 2 \| \M{V}^\T \eve{j}{q} \|^2
  =
  1 -  \| \M{V}^\T \eve{j}{q} \|^2,
\end{displaymath}
which establishes \eqref{eq:22}.

Let $k$ be the rank of $\M{K}$.  We claim that
\begin{equation}
  \label{eq:23}
  k =
  \begin{cases}
    r  & \text{if $\ve{q} \neq \0$},
    \\
    r -1 & \text{otherwise}.
  \end{cases}
\end{equation}
Indeed, expanding out the expression for $\M{K}$, we obtain
\begin{equation}
  \label{eq:24}
  \M{K}
  =
  \begin{bmatrix}
    \M{S}
    \left(
      \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
    \right)
    &
    - \sqrt{1 - \| \M{V}^\T \eve{j}{q} \|^2} \M{S}  \M{V}^\T \eve{j}{q} 
    \\
    \0^\T & 0 
  \end{bmatrix}.
\end{equation}
Since the last row of $\M{K}$ is zero and $\M{K}$ is a $(r+1) \times (r+1)$ matrix, we have $k \leq r$. If $\ve{q} \neq 0$, then, in view of \eqref{eq:22},
\begin{displaymath}
   1 - \| \M{V}^\T \eve{j}{q} \|^2
  =
  \| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|^2
  \neq 0
\end{displaymath}
and, moreover,
\begin{math}
  \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
\end{math}
is invertible, with inverse
\begin{displaymath}
  \left(
    \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
  \right)^{-1}
  =
  \M{I}_r
  +
  \frac{1}{1 - \| \M{V}^\T \eve{j}{q} \|^2}
  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T.
\end{displaymath}
Since $\M{S}$ is invertible, this implies that
\begin{math}
  \M{S}
    \left(
      \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
    \right)
\end{math}
is invertible. As, in view of \eqref{eq:24}, 
\begin{math}
  \M{S}
    \left(
      \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
    \right)
\end{math}
is a $r \times r$ submatrix of $\M{K}$, we have $k \geq r$. Combining this inequality together with the converse inequality established earlier yields $k = r$. If $\ve{q} = \0$, then
\begin{math}
  \| \M{V}^\T \eve{j}{q} \| = 1.
\end{math}
In particular,
\begin{equation}
  \label{eq:25}
   \M{K}
  =
  \begin{bmatrix}
    \M{S}
    \left(
      \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
    \right)
    &
    \0
    \\
    \0^\T & 0 
  \end{bmatrix}.
\end{equation}
Moreover,
\begin{displaymath}
  \M{S}
  \left(
    \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
  \right)
  \M{V}^\T \eve{j}{q}
  =
  \M{S}
  \left(
  \M{V}^\T \eve{j}{q}
  - \| \M{V}^\T \eve{j}{q} \|^2 \M{V}^\T \eve{j}{q}
  \right)
  = \0, 
\end{displaymath}
so $\M{V}^\T \eve{j}{q}$ is a null vector of
\begin{math}
   \M{S}
    \left(
      \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
    \right).
  \end{math}
Therefore
\begin{math}
  \M{S}
  \left(
    \M{I}_r -  \M{V}^\T \eve{j}{q} \big(\M{V}^\T \eve{j}{q} \big)^\T
  \right)
\end{math}
is singular, and as such has rank no greater than $r - 1$. In view of \eqref{eq:25}, this implies that $k \leq r-1$. On the other hand, since the rank of $\zercol{X}{j}$ is at least $r - 1$ and since, on account of \eqref{eq:21}, the rank of $\zercol{X}{j}$ is no greater than $k$, we have $r -1 \leq k$. Thus, in fact, $k = r - 1$. The claim is established.

Let
\begin{equation}
  \label{eq:26}
  \M{K} = \M{C} \M{S}' \M{D}^\T
\end{equation}
be the thin SVD of $\M{K}$, with $\M{C} \in \Mat{(r+1)}{k}$, $\M{S}' \in \Mat{k}{k}$, and $\M{D} \in \Mat{(r+1)}{k}$. Let $\M{U}' \in \Mat{p}{k}$ be given by
\begin{equation}
  \label{eq:27}
  \M{U}' = [\M{U}, \0] \M{C},
\end{equation}
let $\M{V}' \in \Mat{q}{k}$ be given by
\begin{equation}
  \label{eq:28}
  \M{V}' = [\M{V}, \ve{q}] \M{D},
\end{equation}
and, supposing that $q \geq 2$, let $\M{V}'' \in \Mat{(q-1)}{k}$ be given by
\begin{equation}
  \label{eq:29}
  \M{V}'' = \delrow{V'}{j}.
\end{equation}

\begin{result}
  \label{res:2}
  We have
  \begin{equation}
    \label{eq:30}
    \zercol{X}{j} = \M{U}' \M{S}' \M{V}'^\T,
  \end{equation}
  and this representation is the thin SVD of $\zercol{X}{j}$.
\end{result}

\begin{proof}
  Formula~\eqref{eq:30} follows immediately from combining \eqref{eq:21}, \eqref{eq:26}, \eqref{eq:27}, and \eqref{eq:28}.  Since $\M{S}'$ is a diagonal matrix with positive elements in descending order, it remains only show that $\M{U}'$ and $\M{V}' $ have unit-norm, orthogonal columns, that is,
  \begin{math}
    \M{U}'^\T \M{U}' = \M{I}_k
  \end{math}
  and
  \begin{math}
    \M{V}'^\T \M{V}' = \M{I}_k.
  \end{math}
  
  Note that
  \begin{equation}
    \label{eq:31}
    \begin{split}
      \M{U}'^\T \M{U}' & = \M{C}^\T [\M{U}, \0]^\T [\M{U}, \0] \M{C}
      = \M{C}^\T
      \begin{bmatrix}
        \M{U}^\T \M{U} & \0 \\
        \0^\T & 0
      \end{bmatrix}
      \M{C}
      \\
      & = \M{C}^\T
      \begin{bmatrix}
        \M{I}_r & \0\\
        \0^\T & 0
      \end{bmatrix}
      \M{C}.
    \end{split}
  \end{equation}
  Since, as is apparent from \eqref{eq:24}, each column of $\M{K}$ has zero last entry and since the column space of $\M{C}$ coincides with the column space of $\M{K}$, it follows that each column of $\M{C}$ has zero last entry. In other words,
  \begin{equation}
    \label{eq:32}
  \M{C} =
  \begin{bmatrix}
    \M{C}^{1:r, :}
    \\
    \0^\T
  \end{bmatrix},
\end{equation}
where $\M{C}^{1:r, :} = \M{C}^{1:r,1:k} \in \Mat{r}{k}$.  Therefore
  \begin{align*}
    \M{C}^\T
    \begin{bmatrix}
      \M{I}_r & \0 \\
      \0^\T & 0
    \end{bmatrix}
             \M{C} & =
                     [(\M{C}^{1:r, :})^\T, \0]
                     \begin{bmatrix}
                       \M{I}_r & \0 \\
                       \0^\T & 0
                     \end{bmatrix}
                              \begin{bmatrix}
                                \M{C}^{1:r,:}
                                \\
                                \0^\T
                              \end{bmatrix}
    \\ & =
                [(\M{C}^{1:r, :})^\T, \0]
                \begin{bmatrix}
                  \M{C}^{1:r,:}
                  \\
                  \0^\T
                \end{bmatrix}
    = \M{C}^\T \M{C} = \M{I}_k.
  \end{align*}
  Combining this with \eqref{eq:31} yields
    \begin{math}
      \M{U}'^\T \M{U'} = \M{I}_k.
    \end{math}

  Next note that
  \begin{equation}
    \label{eq:33}
    \begin{split}
    \M{V}'^\T \M{V}'
    & =  \M{D}^\T [\M{V}, \ve{q}]^\T [\M{V}, \ve{q}] \M{D} =
    \M{D}^\T
    \begin{bmatrix}
        \M{V}^\T \M{V} & \M{V}^\T \ve{q} \\
        \ve{q}^\T \M{V} & \| \ve{q} \|^2
      \end{bmatrix}
      \M{D}
      \\
      & = \M{D}^\T
      \begin{bmatrix}
        \M{I}_r & \M{V}^\T \ve{q} \\
        \ve{q}^\T \M{V} & \| \ve{q} \|^2
      \end{bmatrix}
      \M{D}.
  \end{split}
\end{equation}
  If $\ve{q} = \0$, then, obviously, $\M{V}^\T \ve{q} = \0$.  If $\ve{q} \neq \0$, then
  \begin{displaymath}
    \M{V}^\T \ve{q}
    =
    \frac{\M{V}^\T\eve{j}{q} - \M{V}^\T\M{V} \M{V}^\T \eve{j}{q}}{\| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|}
    =
    \frac{\M{V}^\T\eve{j}{q} - \M{I}_r \M{V}^\T \eve{j}{q}}{\| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|}
    =
    \frac{\M{V}^\T\eve{j}{q} - \M{V}^\T \eve{j}{q}}{\| \eve{j}{q} - \M{V} \M{V}^\T \eve{j}{q}\|}
    =
    \0.
  \end{displaymath}
  Thus in either case $\M{V}^\T \ve{q} = \0$. Moreover, $\ve{q}^\T \M{V} = (\M{V}^\T \ve{q})^\T = \0^\T$. Hence
  \begin{displaymath}
    \begin{bmatrix}
      \M{I}_r & \M{V}^\T \ve{q} \\
      \ve{V}^\T \M{U} & \| \ve{q} \|^2
    \end{bmatrix}
    =
    \begin{bmatrix}
      \M{I}_r & \0 \\
      \0^\T& \| \ve{q} \|^2
    \end{bmatrix}.
  \end{displaymath}
  We now consider two cases.
  \begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
    \item Suppose that $\ve{q} \neq \0$ (in which case $k = r$). Then $\| \ve{q} \| = 1$ and
    \begin{equation}
      \label{eq:34}
      \begin{bmatrix}
        \M{I}_r & \M{V}^\T \ve{q}
        \\
        \ve{q}^\T \M{V} & \| \ve{q} \|^2
      \end{bmatrix}
      =
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T& 1
      \end{bmatrix}.
    \end{equation}
    Moreover,
    \begin{displaymath}
      \M{D}^\T
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T& 1
      \end{bmatrix}
      \M{D}
      =
      \M{D}^\T \M{I}_{r+1} \M{D}
      =
      \M{D}^\T \M{D}
      = \M{I}_k.
    \end{displaymath}
    Combining this with \eqref{eq:33} and \eqref{eq:34}, we see that
    \begin{math}
      \M{V}'^\T \M{V}' = \M{I}_k.
    \end{math}
  \item Suppose that $\ve{q} = \0$ (in which case $k = r-1$). Then
    \begin{equation}
      \label{eq:35}
      \begin{bmatrix}
        \M{I}_r & \M{V}^\T \ve{q} \\
        \ve{q}^\T \M{V} & \| \ve{q} \|^2
      \end{bmatrix}
      =
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T & 0
      \end{bmatrix}.
    \end{equation}
    Since, on account of \eqref{eq:25}, each row of $\M{K}$ has zero last entry and since the column space of $\M{D}$ coincides with the transposed row space of $\M{K}$, it follows that each column of $\M{D}$ has zero last entry. In other words,
    \begin{equation}
      \label{eq:36}
      \M{D}
      =
      \begin{bmatrix}
        \M{D}^{1:r, :}
        \\
        \0^\T
      \end{bmatrix},
    \end{equation}
    where $\M{D}^{1:r, :} = \M{D}^{1:r,1:k} \in \Mat{r}{k}$.  Consequently,
    \begin{align*}
      \M{D}^\T
      \begin{bmatrix}
        \M{I}_r & \0 \\
        \0^\T & 0
      \end{bmatrix}
               \M{D}
             & =
               [(\M{D}^{1:r, :})^\T, \0]
               \begin{bmatrix}
                 \M{I}_r & \0 \\
                 \0^\T & 0
               \end{bmatrix}
                        \begin{bmatrix}
                          \M{D}^{1:r, :}
                          \\
                          \0^\T
                        \end{bmatrix}
      \\ & =
                [(\M{D}^{1:r, :})^\T, \0]
                \begin{bmatrix}
                  \M{D}^{1:r, :}
                  \\
                  \0^\T
                \end{bmatrix}
    = \M{D}^\T \M{D} = \M{I}_k.
    \end{align*} 
    Combining this with \eqref{eq:33}, \eqref{eq:35}, we conclude that
    \begin{math}
      \M{V}'^\T \M{V}' = \M{I}_k.
    \end{math}
  \end{enumerate}
  Thus in either of the considered cases, 
  \begin{math}
    \M{V}'^\T \M{V}' = \M{I}_k.
  \end{math}
\end{proof}

An immediate consequence of Result~\ref{res:2} is the following fact.

\begin{result}
  \label{res:3}
  Suppose that $q \geq 2$. We have
  \begin{equation}
    \label{eq:37}
    \delcol{X}{j} = \M{U}' \M{S}' \M{V}''^\T,
  \end{equation}
  and this representation is the thin SVD of $\delcol{X}{j}$.
\end{result}

\begin{proof}
  In accordance with \eqref{eq:20},
  \begin{equation}
    \label{eq:38}
    \delcol{X}{j} = \M{X} \sel{j}{q}^\T.
  \end{equation}
  Since, as one directly verifies,
  \begin{displaymath}
    \sel{j}{q} \eve{j}{q} = \0,
  \end{displaymath}
  we have
  \begin{displaymath}
    \M{X} \sel{j}{q}^\T
    =
    \M{X} \sel{j}{q}^\T
    - \M{X} \eve{j}{q} (\sel{j}{q} \eve{j}{q})^\T
    = 
    (\M{X} - \M{X} \eve{j}{q} (\eve{j}{q})^\T ) \sel{j}{q}^\T.
  \end{displaymath}
  By \eqref{eq:19},
  \begin{displaymath}
    \zercol{X}{j} = \M{X} - \M{X} \eve{j}{q} (\eve{j}{q})^\T,
  \end{displaymath}
  and so
  \begin{displaymath}
    \M{X} \sel{j}{q}^\T =  (\zercol{X}{j}) \sel{j}{q}^\T.
  \end{displaymath}
  On account of \eqref{eq:38}, this can be restated as
  \begin{displaymath}
    \delcol{X}{j} = (\zercol{X}{j}) \sel{j}{q}^\T.
  \end{displaymath}
  Combining the above equality with \eqref{eq:30}, we see that
  \begin{equation}
    \label{eq:39}
    \delcol{X}{j} = \M{U}' \M{S}' \M{V}'^\T \sel{j}{q}^\T
    = \M{U}' \M{S}'
    (\sel{j}{q} \M{V}')^\T.
  \end{equation}
  In view of \eqref{eq:20} and \eqref{eq:29},
  \begin{displaymath}
    \M{V}'' = \delrow{V'}{j} = \sel{j}{q} \M{V}'.
  \end{displaymath}
  Therefore \eqref{eq:39} immediately yields \eqref{eq:37}. Since
  \begin{math}
    \M{U}'^\T \M{U}' = \M{I}_k
  \end{math}
  and since $\M{S}'$ is a diagonal matrix with positive elements in descending order, to conclude the proof, we need only show that
  \begin{math}
    \M{V}''^\T \M{V}'' = \M{I}_k.
  \end{math}

  To this end, note that
  \begin{displaymath}
    \M{V}''^\T  \M{V}'' = \M{V}'^\T \sel{j}{q} \sel{j}{q}^\T \M{V}'.
  \end{displaymath}
  One verifies at once that
  \begin{displaymath}
    \sel{j}{q} \sel{j}{q}^\T = \M{I}_q -  \eve{j}{q} (\eve{j}{q})^\T.
  \end{displaymath}
  Since the column space of $\M{V}'$ coincides with the transposed row space of $\zercol{X}{j}$ and since the transpose of $\zercol{X}{j}$ has zero $j$th row, $\M{V}'$ has zero $j$th row. In other words,
  \begin{displaymath}
    (\eve{j}{q})^\T \M{V}' = \0^\T.
  \end{displaymath}
  Consequently,
  \begin{displaymath}
    \M{V}'^\T \sel{j}{q} \sel{j}{q}^\T \M{V}'
    =
    \M{V}'^\T (\M{I}_q -  \eve{j}{q} (\eve{j}{q})^\T) \M{V}'
    =
    \M{V}'^\T \M{V}'^\T.
  \end{displaymath}
  But
  \begin{math}
    \M{V}'^\T \M{V}'^\T = \M{I}_k.
  \end{math}
  Therefore
  \begin{math}
    \M{V}'^\T \sel{j}{q} \sel{j}{q}^\T \M{V}' = \M{I}_k,
  \end{math}
  which is the same as
  \begin{math}
    \M{V}''^\T \M{V}'' = \M{I}_k.
  \end{math}
\end{proof}

\begin{remark}
  \label{rmk:2}
  Comparing \eqref{eq:26} and \eqref{eq:30}, we see that the SVD of $\M{K}$ and the SVD of $\zercol{X}{j}$ share the diagonal part $\M{S}'$.  It follows that $\M{K}$ and $\zercol{X}{j}$ have the same rank. A moment's reflection shows that
  \begin{equation}
    \label{eq:40}
    \rk \big( \delcol{X}{j} \big)  =
    \begin{cases}
      r & \text{if $\M{X}^{:,j} \in \col \big( \zercol{X}{j} \big)$},
      \\
      r - 1 & \text{otherwise}.
    \end{cases}
  \end{equation}
  Here $\M{X}^{:,j}$ denotes the $j$th column of $\M{X}$ and $r$ is the rank of $\M{X}$. We shall shortly show that
  \begin{align}
    \label{eq:41}
    \M{X}^{:,j} \notin \col (\zercol{X}{j})
    \quad
    \text{if and only if}
    \quad
    \eve{j}{q} = \M{V} \M{V}^\T \eve{j}{q}.
  \end{align}
  Clearly, $\eve{j}{q} = \M{V} \M{V}^\T \eve{j}{q}$ holds if and only if
  \begin{math}
    \ve{q} = \0.
  \end{math}
  Thus \eqref{eq:40}  is equivalent to \eqref{eq:23}. In other words, $k = r$ if the $j$th column of $\M{X}$ is in the linear span of the columns of $\zercol{X}{j}$, and $k = r-1$ if the $j$th column of $\M{X}$ is outside the linear span of the columns of $\zercol{X}{j}$. Note that the linear span of the columns of $\zercol{X}{j}$ is the same as the linear span of all but $j$th column of~$\M{X}$.

  We now proceed to establish equivalence \eqref{eq:41}.
  \begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
  \item Suppose first that $\M{X}^{:,j} \notin \col \big( \zercol{X}{j} \big)$. Then there exists a vector $\ve{f} \in \Mat{p}{1}$ which is orthogonal to $\col \big( \zercol{X}{j} \big)$, but not orthogonal to $\M{X}^{:,j}$. In view of \eqref{eq:19} and the fact that $\M{X}^{:,j}$ is the same as $\M{X}\eve{j}{q}$, the properties of $\ve{f}$ can be expressed as
    \begin{equation}
      \label{eq:42}
      \ve{f}^\T \big(\M{X} - \M{X} \eve{j}{q} (\eve{j}{q})^\T \big) = \0^\T
      \quad
      \text{and}
      \quad
      \ve{f}^\T \M{X}\eve{j}{q} \neq 0.
    \end{equation}
    By taking the transpose, the first of these relations can be written as
    \begin{displaymath}
      \big( \M{X}^\T -  \eve{j}{q} (\eve{j}{q})^\T \M{X}^\T \big) \ve{f} = \0,
    \end{displaymath}
    and hence as
    \begin{equation}
      \label{eq:43}
      \M{X}^\T \ve{f}
      =
      \big( (\eve{j}{q})^\T \M{X}^\T \ve{f} \big) \eve{j}{q}.
    \end{equation}
    Taking into account that
    \begin{math}
      (\eve{j}{q})^\T \M{X}^\T \ve{f} = \ve{f}^\T \M{X}\eve{j}{q}
    \end{math}
    and using the second relation of \eqref{eq:42}, we may meaningfully set
    \begin{displaymath}
      \ve{g} = \big( (\eve{j}{q})^\T \M{X}^\T \ve{f} \big)^{-1} \ve{f}.
    \end{displaymath}
    Then \eqref{eq:43} becomes
    \begin{displaymath}
      \eve{j}{q} = \M{X}^\T \ve{g}.
    \end{displaymath}
    Now, taking into account that, by \eqref{eq:1}, $\M{X}^\T = \M{V} \M{S} \M{U}^\T$, and that $\M{V}^\T \M{V} = \M{I}_r$, we obtain
    \begin{align*}
      \M{V} \M{V}^\T \eve{j}{q}
      & =
        \M{V} \M{V}^\T \M{X}^\T \ve{g}
        =
        \M{V} \M{V}^\T \M{V} \M{S} \M{U}^\T \ve{g}
      \\
      & =
        \M{V} \M{I}_r \M{S} \M{U}^\T \ve{g}
        =
        \M{V} \M{S} \M{U}^\T \ve{g}
        =
        \M{X}^\T \ve{g}
        =
        \eve{j}{q}.
    \end{align*}
    This establishes the $\Rightarrow$ part of equivalence \eqref{eq:41}.
  \item Suppose next that $\eve{j}{q} = \M{V} \M{V}^\T \eve{j}{q}$. Then
    \begin{displaymath}
      \M{V} \M{V}^\T \eve{j}{q}
      =
      \M{V} \M{S} \M{U}^\T \M{U} \M{S}^{-1} \M{V}^\T \eve{j}{q}
      =
      \M{X}^\T \M{U} \M{S}^{-1} \M{V}^\T \eve{j}{q}.
    \end{displaymath}
    Setting
    \begin{math}
      \ve{h} = \M{U} \M{S}^{-1} \M{V}^\T \eve{j}{h},
    \end{math}
    we have
    \begin{displaymath}
      \eve{j}{q} = \M{X}^\T \ve{h}.
    \end{displaymath}
    Hence
    \begin{equation}
      \label{eq:44}
      (\eve{j}{q})^\T \M{X}^\T \ve{h}
      =
      (\eve{j}{q})^\T \eve{j}{q}
      =
      \| \eve{j}{q} \|^2
      = 1.
    \end{equation}
    Moreover,
    \begin{displaymath}
      \M{X}^\T \ve{h}
      = \eve{j}{q}
      = \big((\eve{j}{q})^\T \M{X}^\T \ve{h} \big) \eve{j}{q},
    \end{displaymath}
    which can be expressed as
    \begin{displaymath}
      \big( \M{X}^\T -  \eve{j}{q} (\eve{j}{q})^\T \M{X}^\T \big) \ve{h} = \0,
    \end{displaymath}
    and further as
    \begin{displaymath}
      \ve{h}^\T \big(\M{X} - \M{X} \eve{j}{q} (\eve{j}{q})^\T \big) = \0^\T.
    \end{displaymath}
    Thus $\ve{h}$ is orthogonal to $\col \big( \zercol{X}{j} \big)$. On the other hand, since, by \eqref{eq:44},
    \begin{displaymath}
      \ve{h}^\T \M{X}\eve{j}{q}
      =
      (\eve{j}{q})^\T \M{X}^\T \ve{h}
      =
      1
    \end{displaymath}
    and since $\M{X}^{:,j} = \M{X}\eve{j}{q}$, we see that $\ve{h}$ is not orthogonal to $\M{X}^{:,j}$. Consequently, $\M{X}^{:,j} \notin \col (\zercol{X}{j})$. The $\Leftarrow$ part of equivalence \eqref{eq:41} is established, and so is the entire equivalence.
  \end{enumerate}
\end{remark}

\subsection{Calculating $\M{U}$ and $\M{V}$ iteratively}

Suppose that the matrices $\M{U}$ and $\M{V}$ appearing in \eqref{eq:1} can be represented as
\begin{equation}
  \label{eq:45}
  \M{U} = \Mt{U} \Mt{C},
\end{equation}
where $\Mt{U} \in \Mat{p}{r}$ and $\Mt{C} \in \Mat{r}{r}$, and
\begin{equation}
  \label{eq:46}
  \M{V} = \Mt{V} \Mt{D}, 
\end{equation}
where $\Mt{V} \in \Mat{q}{r}$ and $\Mt{D} \in \Mat{r}{r}$.  Then, as it turns out, the matrices $\M{U}'$ and $\M{V}'$ given in \eqref{eq:27} and \eqref{eq:28} can also be represented in a similar fashion as
\begin{equation}
  \label{eq:47}
  \M{U}' = \Mt{U} \Mt{C}', 
\end{equation}
where $\Mt{C}' \in \Mat{r}{k}$, and
\begin{equation}
  \label{eq:48}
  \M{V}' = \Mt{V}' \Mt{D}', 
\end{equation}
where $\Mt{V}' \in \Mat{q}{(k+1)}$ and $\Mt{D}' \in \Mat{(k+1)}{k}$.  Moreover, supposing that $q \geq 2$, if we let
\begin{equation}
  \label{eq:49}
  \Mt{V}'' =  \delrow{\Mt{V}'}{j},
\end{equation}
then, on account of \eqref{eq:48}, the matrix $\M{V}''$ given in \eqref{eq:29} can be represented as
\begin{equation}
  \label{eq:50}
  \M{V}''= \Mt{V}'' \Mt{D}'.
\end{equation}
The explicit formulae for $\Mt{C}'$, $\Mt{V}'$, and $\Mt{D}'$, which we shall derive next, are intended to be used iteratively to obtain an expression for the thin SVD of a matrix formed by zeroing in succession a number of columns to a given matrix, and to obtain a companion expression for the thin SVD of a matrix formed by subtracting in succession a number of columns to a given matrix.

\subsubsection{A formula for $\M{U}'$}

In view of \eqref{eq:27}, \eqref{eq:32}, and \eqref{eq:45},
\begin{displaymath}
  \M{U}'
  % = [\M{U}, \0] \M{C}
  = [\M{U}, \0]
  \begin{bmatrix}
    \M{C}^{1:r, :}
    \\
    \0^\T
  \end{bmatrix}
  = \M{U} \M{C}^{1:r, :}
  = \Mt{U} \Mt{C}  \M{C}^{1:r, :}.
\end{displaymath}
We now obtain the desired representation
\begin{math}
  \M{U}' = \Mt{U} \Mt{C}'
\end{math}
by letting
\begin{displaymath}
  % \Mt{U}' = \Mt{U}
  % \quad
  % \text{and}
  % \quad
  \Mt{C}' = \Mt{C}  \M{C}^{1:r, :}.
\end{displaymath}
Since $\Mt{C} \in \Mat{r}{r}$ and $\M{C}^{1:r, :} \in \Mat{r}{k}$, it is clear that $\Mt{C}' \in \Mat{r}{k}$. Moreover, since $k = r$ if $\ve{q} \neq \0$ and since $k = r - 1$ if $\ve{q} = \0$, the $r \times k$ matrix $\M{C}^{1:r,:}$ takes the form
\begin{displaymath}
  \M{C}^{1:r, :}
  =
  \begin{cases}
    \M{C}^{1:r, 1:r} & \text{if $\ve{q} \neq \0$},
    \\
    \M{C}^{1:r, 1:(r-1)} & \text{otherwise}.
  \end{cases}
\end{displaymath}
Therefore
\begin{displaymath}
  \Mt{C}' 
  =
  \begin{cases}
    \Mt{C}  \M{C}^{1:r, 1:r} & \text{if $\ve{q} \neq \0$},
    \\
    \Mt{C}  \M{C}^{1:r, 1:(r-1)} & \text{otherwise}.
  \end{cases}
\end{displaymath}

\subsubsection{A formula for $\M{V}'$}

We consider two cases.

\begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
\item Suppose that $\ve{q} \neq \0$ (or, equivalently, that $k=r$).  Then,
  in view of \eqref{eq:28} and \eqref{eq:46},
  \begin{displaymath}
    \M{V}'
    % = [\M{V}, \ve{q}] \M{D}
    =
    [\Mt{V} \Mt{D}, \ve{q}] \M{D}
    =
     [\Mt{V}, \ve{q}] 
    \begin{bmatrix}
      \Mt{D} & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \M{D}
  \end{displaymath}
%
%   
% \begin{displaymath}
%   \M{V}' =
%   \begin{bmatrix}
%     \M{V} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}
%   =
%   \begin{bmatrix}
%     \Mt{V} \Mt{D} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}
%   =
%   \begin{bmatrix}
%     \Mt{V} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \begin{bmatrix}
%     \Mt{D} & \0
%     \\
%     \0^\T & 1
%   \end{bmatrix}
%   \M{D}.
% \end{displaymath}
%
%
Accordingly, we obtain the representation
\begin{math}
  \M{V}' = \Mt{V}' \Mt{D}'
\end{math}
by letting
\begin{displaymath}
  \Mt{V}' = [\Mt{V}, \ve{q}]
  \quad
  \text{and}
  \quad
  \Mt{D}' =
  \begin{bmatrix}
    \Mt{D} & \0
    \\
    \0^\T & 1
  \end{bmatrix}
  \M{D}.
\end{displaymath}
\item Suppose that $\ve{q} = \0$ (or, equivalently, that $k = r-1$).  Then $\M{D}$ is a $(r+1) \times (r-1)$ matrix. In particular, $\M{D}^{1:r,1:} = \M{D}^{1:r,1:(r-1)}$ and the identity in \eqref{eq:36} can be written as
  \begin{displaymath}
    \M{D}
    =
    \begin{bmatrix}
      \M{D}^{1:r,1:(r-1)}
      \\
      \0^\T
    \end{bmatrix}.
  \end{displaymath}
  Now, by \eqref{eq:28},
  \begin{displaymath}
    \M{V}' = [\M{V}, \ve{q}] \M{D}
    = [\M{V}, \0]
    \begin{bmatrix}
      \M{D}^{1:r,1:(r-1)}
      \\
      \0^\T
    \end{bmatrix}
    =
    \M{V} \M{D}^{1:r,1:(r-1)}
    =
    \Mt{V} \Mt{D} \M{D}^{1:r,1:(r-1)}.
  \end{displaymath}
  %
  %  
%   \begin{displaymath}
%     \M{V}' =
%     \begin{bmatrix}
%       \M{V} & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix}
%     \M{D}
%     =
%     \begin{bmatrix}
%       \M{V} & \0
%       \\
%       \0^\T & 1
%     \end{bmatrix}
%     \begin{bmatrix}
%       \M{D}^{1:r,1:r}
%       \\
%       \M{D}^{r+1,1:r}
%     \end{bmatrix}
%     =
%     \begin{bmatrix}
%       \M{V} \M{D}^{1:r,1:r}
%       \\
%       \M{D}^{r+1,1:r}
%     \end{bmatrix}.
%   \end{displaymath}
% Furthermore, in view of \eqref{eq:14},  
% \begin{align*}
%   % \M{V}'
%   % & =
%     \begin{bmatrix}
%       \M{V} \M{D}^{1:r,1:r}
%       \\
%       \M{D}^{r+1,1:r}
%     \end{bmatrix}
%   & =
%   \begin{bmatrix}
%     \Mt{V} \Mt{D} \M{D}^{1:r,1:r}
%     \\
%     \M{D}^{r+1,1:r}
%   \end{bmatrix}
%   =
%   \begin{bmatrix}
%     \Mt{V} \Mt{D} \M{D}^{1:r, 1:r}
%     \\
%     \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^+ \Mt{D} \M{D}^{1:r, 1:r}
%   \end{bmatrix}
%   \\
%   & =
%     \begin{bmatrix}
%       \Mt{V}
%       \\
%       \M{D}^{r+1, 1:r} \left(\Mt{D} \M{D}^{1:r,1:r} \right)^+
%     \end{bmatrix}
%   \Mt{D} \M{D}^{1:r, 1:r}.
    %   \end{align*}
    %
  %
This leads to the representation
\begin{math}
  \M{V}' =  \Mt{V}' \Mt{D}'
\end{math}
with
\begin{displaymath}
  \Mt{V}' =  \Mt{V}
  \quad
  \text{and}
  \quad
  \Mt{D}' =
  \Mt{D} \M{D}^{1:r, 1:(r-1)}.
\end{displaymath}
\end{enumerate}

% \begin{displaymath}
%   \delcol{A}{i_1, \dots, i_k}
%   =
%   [ \dots [[\delcol{A}{i_1}] \varominus \mathrm{col}(i_2)]
%    \varominus \mathrm{col}(i_3)
%   \dots ]
%   \varominus \mathrm{col}(i_k)
% \end{displaymath}

\subsection{Iterative zeroing}
\label{sec:iterative-zeroing}

Let $\M{X} \in \Mat{p}{q}$. Let $E$ be a proper subset of $\Brackets{q}$ with $K$ elements,
\begin{math}
  K \in \Brackets{q - 1}.
\end{math}
Let
\begin{math}
  \zercol{X}{E}
\end{math}
denote the $p \times q$ matrix resulting from zeroing the columns of $\M{X}$ with indices in $E$. If $\{j_1, \dots, j_K\}$ is a specific enumeration of $E$ (so that
\begin{math}
  j_1 \in \Brackets{q},
\end{math}
\begin{math}
  j_2 \in \Brackets{q} \setminus \{j_1\},
\end{math}
\dots,
\begin{math}
  j_K \in \Brackets{q} \setminus \{j_1, \dots, j_{K-1}\}
\end{math}%
), then
\begin{math}
  \zercol{X}{E}
\end{math}
can be identified with the $p \times q$ matrix
\begin{math}
  \zercol{X}{j_1, \dots, j_K}
\end{math}
defined by
\begin{displaymath}
  \zercol{X}{j_1, \dots, j_K}
  \colonequals
  [ \dots [[\zercol{X}{j_1}] \varoslash \mathrm{col}(j_2)]
   \varoslash \mathrm{col}(j_3)
  \dots ]
  \varoslash \mathrm{col}(j_K).
\end{displaymath}
This matrix arises by first zeroing the $j_1$th column of $\M{X}$, then zeroing the $j_2$th column of the $p \times q$ matrix obtained in the previous step, and, continuing in this way, finally zeroing the $j_K$th column in the $p \times q$ matrix obtained in the $(K-1)$th step.  The SVD of
\begin{math}
  \zercol{X}{j_1, \dots, j_K}
\end{math}
can be obtained from the SVD of $\M{X}$ by computing successively the SVD of the intermediary matrices
\begin{math}
  \zercol{X}{j_1, \dots, j_k},
\end{math}
$k \in \Brackets{K-1}$.  The process involves the repetitive use of \eqref{eq:47} and \eqref{eq:48}. The following algorithm---inspired by the considerations in \cite{brand06:_fast}, but developed independently here---describes the procedure.

%\pagebreak[1]

\begin{algorithm}[\textnormal{\textsc{ThinSVD}(\unboldmath$\zercol{X}{j_1,j_2, \dots, j_K}$)}]
  \rule{\textwidth}{0.4pt}
  \begin{algorithmic}[1]
    \Require
    \begin{math}
      \M{X} \in \Mat{p}{q}, 
    \end{math}
    \begin{math}
      K \in \Brackets{q - 1},
    \end{math}
    \begin{math}
      j_1 \in \Brackets{q},
    \end{math}
    \begin{math}
      j_2 \in \Brackets{q} \setminus \{j_1\},
    \end{math}
    \dots,
    \begin{math}
      j_K \in \Brackets{q} \setminus \{j_1, \dots, j_{K-1}\},
    \end{math}
    \State $(\M{U}, \M{S}, \M{V}) \gets \textsc{ThinSVD}(\M{X})$
    \State $r \gets \textsc{NumberOfColums/Rows}(\M{S})$
    % \State $\Mt{U} \gets \M{U}$
    \State $\Mt{C} \gets \M{I}_r$
    \State $\Mt{V} \gets \M{V}$
    \State $\Mt{D} \gets \M{I}_r$
    \For{$k \gets 1 \ldots K$}
    \If {$\eve{j_k}{q} - \M{V} \M{V}^\T \eve{j_k}{q} \neq \0$}
    \State
    \begin{math}
    \ve{q} \gets
    \frac{\eve{j_k}{q} - \M{V} \M{V}^\T \eve{j_k}{q}}
    {\| \eve{j_k}{q} - \M{V} \M{V}^\T \eve{j_k}{q}\|}
    \end{math}
    \Else
    \State $\ve{q} \gets \0$
    \EndIf
    \State
    \begin{math}
      \M{K}
      \gets
      \begin{bmatrix}
        \M{S} & \0
        \\
        \0^\T & 0
      \end{bmatrix}
      \left(
        \M{I}
        -
        \begin{bmatrix}
          \M{V}^\T \eve{j_k}{q}
          \\
          0
        \end{bmatrix}
        \begin{bmatrix}
          \M{V}^\T \eve{j_k}{q}
          \\
          \sqrt{1 - \| \M{V}^\T \eve{j_k}{q} \|^2}
        \end{bmatrix}^\T \right)
    \end{math}
    \State
    $(\M{C}, \M{S}', \M{D}) \gets \textsc{ThinSVD}(\M{K})$
    \If {$\ve{q} \neq \0$}
    \State
    \begin{math}
      \Mt{C}
      \gets
      \Mt{C}  \M{C}^{1:r, 1:r}
    \end{math}
    \State
    \begin{math}
      \Mt{V} \gets [\Mt{V}, \ve{q}]
    \end{math}
    \State
    \begin{math}
      \Mt{D}
      \gets
      \begin{bmatrix}
        \Mt{D} & \0
        \\
        \0^\T & 1
      \end{bmatrix}
      \M{D}
    \end{math}
    \Else
    \State
    \begin{math}
      \Mt{C}
      \gets
      \Mt{C}  \M{C}^{1:r, 1:(r-1)}
    \end{math}
    % \State
    % \begin{math}
    %   \Mt{V}
    %   \gets
    %   \delrow{\Mt{V}}{j_k}
    % \end{math}
    \State
    \begin{math}
      \Mt{D}
      \gets
      \Mt{D} \M{D}^{1:r, 1:(r-1)}
    \end{math}
    \EndIf
    \State
    \begin{math}
      \M{V} \gets \Mt{V} \Mt{D}
    \end{math}
    \State
    \begin{math}
      \M{S} \gets \M{S}'
    \end{math}
    \State
    \begin{math}
      r \gets \textsc{NumberOfColumns/Rows}(\M{S})
    \end{math}
    \EndFor
   \State
    \begin{math}
      \M{U} \gets \M{U} \Mt{C}
    \end{math}
     % \begin{math}
    %   \M{U} \gets \Mt{U} \Mt{C}
    % \end{math}
    \State
    \Return $(\M{U}, \M{S}, \M{V})$
  \end{algorithmic}
\end{algorithm}



\subsection{Iterative downdating}
\label{sec:iterative-downdating}

Let $\M{X} \in \Mat{p}{q}$. If
\begin{math}
  K \in \Brackets{q-1},
\end{math}
\begin{math}
  j_1 \in \Brackets{q},
\end{math}
\begin{math}
  j_2 \in \Brackets{q - 1},
\end{math}
\dots,
\begin{math}
  j_K \in \Brackets{q - K + 1},
\end{math}
then one can consider the matrix
\begin{displaymath}
  \delcol{X}{j_1, \dots, j_K}
  \colonequals
  [ \dots [[\delcol{X}{j_1}] \varominus \mathrm{col}(j_2)]
   \varominus \mathrm{col}(j_3)
  \dots ]
  \varominus \mathrm{col}(j_K).
\end{displaymath}
This matrix arises by first deleting the $j_1$th column from $\M{X}$, then deleting the $j_2$th column from the resulting $p \times (q-1)$ matrix, and, continuing in this way, finally deleting the $j_K$th column from the $p \times (q - K + 1)$ matrix obtained in the $(K-1)$th step. The SVD of
\begin{math}
  \delcol{X}{j_1, \dots, j_K}
\end{math}
can be obtained from the SVD of $\M{X}$ by computing successively the SVD of the intermediary matrices
\begin{math}
  \delcol{X}{j_1, \dots, j_k},
\end{math}
$k \in \Brackets{K-1}$.  The process involves the repetitive use of \eqref{eq:47}, \eqref{eq:48}, \eqref{eq:49}, and \eqref{eq:50}.  The following algorithm describes the procedure.

\begin{algorithm}[\textnormal{\textsc{ThinSVD}(\unboldmath$\delcol{X}{j_1,j_2, \dots, j_K}$)}]
  \rule{\textwidth}{0.4pt}
  \begin{algorithmic}[1]
    \Require
    \begin{math}
      \M{X} \in \Mat{p}{q}, 
    \end{math}
    \begin{math}
      K \in \Brackets{q - 1},
    \end{math}
    \begin{math}
      j_1 \in \Brackets{q},
    \end{math}
    \begin{math}
      j_2 \in \Brackets{q - 1},
    \end{math}
    \dots,
    \begin{math}
      j_K \in \Brackets{q - K + 1}
    \end{math}
    \State $(\M{U}, \M{S}, \M{V}) \gets \textsc{ThinSVD}(\M{X})$
    \State $r \gets \textsc{NumberOfColumns/Rows}(\M{S})$
    %\State $\Mt{U} \gets \M{U}$
    \State $\Mt{C} \gets \M{I}_r$
    \State $\Mt{V} \gets \M{V}$
    \State $\Mt{D} \gets \M{I}_r$
    \For{$k \gets 1 \ldots K$}
    \If {$\eve{j_k}{q-k+1} - \M{V} \M{V}^\T \eve{j_k}{q-k+1} \neq \0$}
    \State
    \begin{math}
    \ve{q} \gets
    \frac{\eve{j_k}{q-k+1} - \M{V} \M{V}^\T \eve{j_k}{q-k+1}}
    {\| \eve{j_k}{q-k+1} - \M{V} \M{V}^\T \eve{j_k}{q-k+1}\|}
    \end{math}
    \Else
    \State $\ve{q} \gets \0$
    \EndIf
    \State
    \begin{math}
      \M{K}
      \gets
      \begin{bmatrix}
        \M{S} & \0
        \\
        \0^\T & 0
      \end{bmatrix}
      \left(
        \M{I}
        -
        \begin{bmatrix}
          \M{V}^\T \eve{j_k}{q-k+1}
          \\
          0
        \end{bmatrix}
        \begin{bmatrix}
          \M{V}^\T \eve{j_k}{q-k+1}
          \\
          \sqrt{1 - \| \M{V}^\T \eve{j_k}{q-k+1} \|^2}
        \end{bmatrix}^\T \right)
    \end{math}
    \State
    $(\M{C}, \M{S}', \M{D}) \gets \textsc{ThinSVD}(\M{K})$
    \If {$\ve{q} \neq \0$}
    \State
    \begin{math}
      \Mt{C}
      \gets
      \Mt{C}  \M{C}^{1:r, 1:r}
    \end{math}
    \State
    \begin{math}
      \Mt{V} \gets \delrow{[\Mt{V}, \ve{q}]}{j_k}
    \end{math}
    \State
    \begin{math}
      \Mt{D}
      \gets
      \begin{bmatrix}
        \Mt{D} & \0
        \\
        \0^\T & 1
      \end{bmatrix}
      \M{D}
    \end{math}
    \Else
    \State
    \begin{math}
      \Mt{C}
      \gets
      \Mt{C}  \M{C}^{1:r, 1:(r-1)}
    \end{math}
    \State
    \begin{math}
      \Mt{V}
      \gets
      \delrow{\Mt{V}}{j_k}
    \end{math}
    \State
    \begin{math}
      \Mt{D}
      \gets
      \Mt{D} \M{D}^{1:r, 1:(r-1)}
    \end{math}
    \EndIf
    \State
    \begin{math}
      \M{V} \gets \Mt{V} \Mt{D}
    \end{math}
    \State
    \begin{math}
      \M{S} \gets \M{S}'
    \end{math}
    \State
    \begin{math}
      r \gets \textsc{NumberOfColumns/Rows}(\M{S})
    \end{math}
    \EndFor
   \State
    \begin{math}
      \M{U} \gets \M{U} \Mt{C}
    \end{math}
    % \begin{math}
    %   \M{U} \gets \Mt{U} \Mt{C}
    % \end{math}
    \State
    \Return $(\M{U}, \M{S}, \M{V})$
  \end{algorithmic}
\end{algorithm}

% \begin{algorithm}[\textnormal{\textsc{ThinSVD}(\unboldmath$\zercol{X}{j_1,j_2, \dots, j_K}$)}]
%   \rule{\textwidth}{0.4pt}
%   \begin{algorithmic}[1]
%     \Require
%     \begin{math}
%       \M{X} \in \Mat{p}{q}, 
%     \end{math}
%     \begin{math}
%       K \in \Brackets{q - 1},
%     \end{math}
%     \begin{math}
%       j_1 \in \Brackets{q},
%     \end{math}
%     \begin{math}
%       j_2 \in \Brackets{q} \setminus \{j_1\},
%     \end{math}
%     \dots,
%     \begin{math}
%       j_K \in \Brackets{q} \setminus \{j_1, \dots, j_{K-1}\},
%     \end{math}
%     \State $(\M{U}, \M{S}, \M{V}) \gets \textsc{ThinSVD}(\M{X})$
%     \State $r \gets \textsc{Order}(\M{S})$
%     % \State $\Mt{U} \gets \M{U}$
%     \State $\Mt{C} \gets \M{I}_r$
%     \State $\Mt{V} \gets \M{V}$
%     \State $\Mt{D} \gets \M{I}_r$
%     \For{$k \gets 1 \ldots K$}
%     \If {$\eve{j_k}{q} - \M{V} \M{V}^\T \eve{j_k}{q} \neq \0$}
%     \State
%     \begin{math}
%     \ve{q} \gets
%     \frac{\eve{j_k}{q} - \M{V} \M{V}^\T \eve{j_k}{q}}
%     {\| \eve{j_k}{q} - \M{V} \M{V}^\T \eve{j_k}{q}\|}
%     \end{math}
%     \Else
%     \State $\ve{q} \gets \0$
%     \EndIf
%     \State
%     \begin{math}
%       \M{K}
%       \gets
%       \begin{bmatrix}
%         \M{S} & \0
%         \\
%         \0^\T & 0
%       \end{bmatrix}
%       \left(
%         \M{I}
%         -
%         \begin{bmatrix}
%           \M{V}^\T \eve{j_k}{q}
%           \\
%           0
%         \end{bmatrix}
%         \begin{bmatrix}
%           \M{V}^\T \eve{j_k}{q}
%           \\
%           \sqrt{1 - \| \M{V}^\T \eve{j_k}{q} \|^2}
%         \end{bmatrix}^\T \right).
%     \end{math}
%     \State
%     $(\M{C}, \M{S}', \M{D}) \gets \textsc{ThinSVD}(\M{K})$
%     \If {$\ve{q} \neq \0$}
%     \State
%     \begin{math}
%       \Mt{C}
%       \gets
%       \Mt{C}  \M{C}^{1:r, 1:r}
%     \end{math}
%     \State
%     \begin{math}
%       \Mt{V} \gets [\Mt{V}, \ve{q}]
%     \end{math}
%     \State
%     \begin{math}
%       \Mt{D}
%       \gets
%       \begin{bmatrix}
%         \Mt{D} & \0
%         \\
%         \0^\T & 1
%       \end{bmatrix}
%       \M{D}
%     \end{math}
%     \Else
%     \State
%     \begin{math}
%       \Mt{C}
%       \gets
%       \Mt{C}  \M{C}^{1:r, 1:(r-1)}
%     \end{math}
%     % \State
%     % \begin{math}
%     %   \Mt{V}
%     %   \gets
%     %   \delrow{\Mt{V}}{j_k}
%     % \end{math}
%     \State
%     \begin{math}
%       \Mt{D}
%       \gets
%       \Mt{D} \M{D}^{1:r, 1:(r-1)}.
%     \end{math}
%     \EndIf
%     \State
%     \begin{math}
%       \M{V} \gets \Mt{V} \Mt{D}
%     \end{math}
%     \State
%     \begin{math}
%       \M{S} \gets \M{S}'
%     \end{math}
%     \State
%     \begin{math}
%       r \gets \textsc{Order}(\M{S})
%     \end{math}
%     \EndFor
%    \State
%     \begin{math}
%       \M{U} \gets \M{U} \Mt{C}
%     \end{math}
%      % \begin{math}
%     %   \M{U} \gets \Mt{U} \Mt{C}
%     % \end{math}
%     \State
%     \Return $(\M{U}, \M{S}, \M{V})$
%   \end{algorithmic}
% \end{algorithm}

\begin{comment}

\subsection{Downdating}

Let $\M{X}_0$ be a $p \times q$ matrix, and let
\begin{displaymath}
  \M{X}_0 = \M{U}_0 \M{S}_0 \M{V}_0^\T
\end{displaymath}
be the SVD of $\M{X}_0$.  For $1 \leq k \leq q$, let $\M{X}_k$ be the matrix obtained from $\M{X}_0$ by deleting the last $k$ columns,
\begin{displaymath}
  \M{X}_k = \M{X}_0^{:,1:(q -k)}.
\end{displaymath}
Let
\begin{displaymath}
  \M{X}_k = \M{U}_k \M{S}_k\M{V}_k^\T
\end{displaymath}
be the SVD of $\M{X}_k$. We shall give a recursive formula for $\M{U}_k$, $\M{S}_k$, $\M{V}_k$, and some auxiliary matrices.

Given $n \in \N$ and $i \in \{1,2, \dots, n\}$, let $\eve{i}{n} = [0, \dots, 0, 1, 0, \dots, 0]^\T$ be the length-$n$ vector with $1$ in the $i$-th position and $0$ in all others.

For each $k = 0, 1, \dots, q-1$, let $r_k$ denote the rank of $\M{X}_k$, which is the same as the rank of $\M{S}_k$.

We shall first give a formula for the SVD of $\M{X}_1$. Let
\begin{displaymath}
  \ve{n}_1 = \M{V}_0^\T \eve{q}{q}
  \quad
  \text{and}
  \quad
  \ve{q}_1 =  \frac{\eve{q}{q} - \M{V}_0 \ve{n}_1}{\sqrt{1 - \| \ve{n}_1\|^2}}.
\end{displaymath}
We consider two cases.
\begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
\item Suppose that $\ve{q}_1 \neq \0$. Let
  \begin{displaymath}
    \M{K}_1
    =
    \begin{bmatrix}
      \M{S}_0 & \0
      \\
      \0^\T & 0
    \end{bmatrix}
    \left(
      \M{I}
      -
      \begin{bmatrix}
        \ve{n}_1
        \\
        0
      \end{bmatrix}
      \begin{bmatrix}
        \ve{n}_1
        \\
        \sqrt{1 - \| \ve{n}_1 \|^2}
      \end{bmatrix}^\T \right).
  \end{displaymath}
  Let
  \begin{displaymath}
    \M{K}_1 = \M{C}_1 \M{S}_1  \M{D}_1^\T
  \end{displaymath}
  be the SVD of $\M{K}_1$.  Then the matrix $\M{C}_1$ takes the form
  \begin{displaymath}
    \M{C}_1
    =
    \begin{bmatrix}
      \M{C}_1^{1:r_0, :}
      \\
      \0^\T
    \end{bmatrix}.
  \end{displaymath}
  Set
  \begin{displaymath}
    \Mt{C}_1 = \M{C}^{1:r_0,:}_1,
    \quad
    \Mt{V}_1 = [\M{V}_0, \ve{q}_1]^{1:(q - 1),:}
    \quad
    \text{and}
    \quad
    \Mt{D}_1 = \M{D}_1.
  \end{displaymath}
  Here $[\M{V}_0, \ve{q}_1]^{1:(q - 1),:}$ stands for the matrix $[\M{V}_0, \ve{q}_1]$ with the last row deleted. The SVD of $\M{X}_1$ is given by
  \begin{displaymath}
    \M{X}_1 = \M{U}_1 \M{S}_1\M{V}_1^\T,
  \end{displaymath}
  where
  \begin{displaymath}
    \M{U}_1 = \M{U}_0 \Mt{C}_1
    \quad
    \text{and}
    \quad
    \M{V}_1 = \Mt{V}_1 \Mt{D}_1.
  \end{displaymath}
\item Suppose that $\ve{q}_1 = \0$. Let
  \begin{displaymath}
    \M{K}_1
    =
    \M{S}_0 - \M{S}_0 \ve{n}_1 \ve{n}_1^\T
    =
    \M{S}_0 \left( \M{I} -  \ve{n}_1 \ve{n}_1^\T \right).
  \end{displaymath}
  Let
  \begin{displaymath}
    \M{K}_1 = \M{C}_1 \M{S}_1  \M{D}_1^\T
  \end{displaymath}
  be the SVD of $\M{K}_1$.  Set
  \begin{displaymath}
    \Mt{C}_1 = \M{C}_1,
    \quad
    \Mt{V}_1 = \M{V}_0^{1:(q - 1),:}
    \quad
    \text{and}
    \quad
    \Mt{D}_1 = \M{D}_1.
  \end{displaymath}
  Here $\M{V}_0^{1:(q - 1),:}$ stands for the matrix $\M{V}_0$ with the last row deleted. The SVD of $\M{X}_1$ is given by
  \begin{displaymath}
    \M{X}_1 = \M{U}_1 \M{S}_1\M{V}_1^\T,
  \end{displaymath}
  where
  \begin{displaymath}
    \M{U}_1 = \M{U}_0 \Mt{C}_1
    \quad
    \text{and}
    \quad
    \M{V}_1 = \Mt{V}_1 \Mt{D}_1.
  \end{displaymath}
\end{enumerate}

We shall now give a formula for the SVD of $\M{X}_{k+1}$. Suppose that $\Mt{C}_k$, $\Mt{V}_k$, $\Mt{D}_k$, $\M{S}_k$, and $\M{V}_k$ are known. Let
\begin{displaymath}
  \ve{n}_{k+1} = \M{V}_k^\T \eve{q-k}{q-k}
  \quad
  \text{and}
  \quad
  \ve{q}_{k+1} =  \frac{\eve{q-k}{q-k} - \M{V}_k \ve{n}_{k+1}}{\sqrt{1 - \| \ve{n}_{k+1}\|^2}}.
\end{displaymath}
We consider two cases.
\begin{enumerate}[font=\upshape,label=(\roman*),wide,align=right]
\item Suppose that $\ve{q}_{k+1} \neq \0$. Let
  \begin{displaymath}
    \M{K}_{k+1}
    =
    \begin{bmatrix}
      \M{S}_k & \0
      \\
      \0^\T & 0
    \end{bmatrix}
    \left(
      \M{I}
      -
      \begin{bmatrix}
        \ve{n}_{k+1}
        \\
        0
      \end{bmatrix}
      \begin{bmatrix}
        \ve{n}_{k+1}
        \\
        \sqrt{1 - \| \ve{n}_{k+1} \|^2}
      \end{bmatrix}^\T \right).
  \end{displaymath}
  Let
  \begin{displaymath}
    \M{K}_{k+1} = \M{C}_{k+1} \M{S}_{k+1}  \M{D}_{k+1}^\T
  \end{displaymath}
  be the SVD of $\M{K}_{k+1}$.  Then the matrix $\M{C}_{k+1}$ takes the form
  \begin{displaymath}
    \M{C}_{k+1}
    =
    \begin{bmatrix}
      \M{C}_{k+1}^{1:r_k, :}
      \\
      \0^\T
    \end{bmatrix}.
  \end{displaymath}
  Set
  \begin{displaymath}
    \Mt{C}_{k+1} = \Mt{C}_k \M{C}^{1:r_k, :}_{k+1},
    \quad
    \Mt{V}_{k+1}
    =
    [\Mt{V}_k,  \ve{q}_{k+1}]^{1:(q-k-1),:},
  \end{displaymath}
  and
  \begin{displaymath}
    \Mt{D}_{k+1}
    =
    \begin{bmatrix}
      \Mt{D}_k & \0
      \\
      \0^\T & 1
    \end{bmatrix}
    \M{D}_{k+1}.
  \end{displaymath}
  Here $[\Mt{V}_k, \ve{q}_{k+1}]^{1:(q - k - 1),:}$ stands for the matrix $[\Mt{V}_k, \ve{q}_{k+1}]$ with the last row deleted.  The SVD of $\M{X}_{k+1}$ is given by
  \begin{displaymath}
    \M{X}_{k+1} = \M{U}_{k+1} \M{S}_{k+1}\M{V}_{k+1}^\T,
  \end{displaymath}
  where
  \begin{displaymath}
    \M{U}_{k+1} = \M{U}_0 \Mt{C}_{k+1}
    \quad
    \text{and}
    \quad
    \M{V}_{k+1} = \Mt{V}_{k+1} \Mt{D}_{k+1}.
  \end{displaymath}
\item Suppose that $\ve{q}_{k+1} =\0$. Let
  \begin{displaymath}
    \M{K}_{k+1}
    = \M{S}_k - \M{S}_k \ve{n}_{k+1} \ve{n}_{k+1}^\T
    = \M{S}_k \left( \M{I} - \ve{n}_{k+1} \ve{n}_{k+1}^\T \right).
  \end{displaymath}
  Let
  \begin{displaymath}
    \M{K}_{k+1} = \M{C}_{k+1} \M{S}_{k+1}  \M{D}_{k+1}^\T
  \end{displaymath}
  be the SVD of $\M{K}_{k+1}$.  Set
  \begin{displaymath}
    \Mt{C}_{k+1} = \Mt{C}_k \M{C}_{k+1},
    \quad
    \Mt{V}_{k+1}
    =
    \Mt{V}_k^{1:(q-k-1),:},
    \quad
    \text{and}
    \quad
    \Mt{D}_{k+1}
    = \Mt{D}_k \M{D}_{k+1}.
  \end{displaymath}
  Here $\Mt{V}_k^{1:(q - k - 1),:}$ stands for the matrix $\Mt{V}_k$ with the last row deleted.  The SVD of $\M{X}_{k+1}$ is given by
  \begin{displaymath}
    \M{X}_{k+1} = \M{U}_{k+1} \M{S}_{k+1}\M{V}_{k+1}^\T,
  \end{displaymath}
  where
  \begin{displaymath}
    \M{U}_{k+1} = \M{U}_0 \Mt{C}_{k+1}
    \quad
    \text{and}
    \quad
    \M{V}_{k+1} = \Mt{V}_{k+1} \Mt{D}_{k+1}.
  \end{displaymath}

\end{enumerate}

% \begin{align*}
%   \M{C}_1
%   & =
%     \begin{bmatrix}
%       \M{C}_1^{1:r_0, :}
%       \\
%       \0^\T
%     \end{bmatrix},
%   %     =
%   %     \begin{bmatrix}
%   %       \M{C}^{1:r_0, 1:r_1}
%   %       \\
%   %       \0^\T
%   %     \end{bmatrix},
%   \\[2ex]
%   \M{S}_1
%   & =
%     \diag(s_1^{(1)}, \dots, s_{r_1}^{(1)}),
%   \\[2ex]
%   \M{D}_1
%   & =
%     \begin{bmatrix}
%       \M{D}^{1:r_0, 1:r_1}_1
%       \\
%       \M{D}^{r_0+1, 1:r_1}_1
%     \end{bmatrix}.
% \end{align*}

\end{comment}

\begin{comment}

\begin{displaymath}
  \M{U}_1 = \M{U}_0 \Mt{C}_1
\end{displaymath}

Let
\begin{displaymath}
  \Mt{V}_1 = [\M{V}_0, \ve{q}_1]^{1:(q_0 - 1),:}
  \quad
  \text{and}
  \quad
  \Mt{D}_1 = \M{D}_1,
\end{displaymath}

\begin{displaymath}
  \M{X}_1 = \M{U}_1 \M{S}_1 \Mt{V}_1 \Mt{D}_1.
\end{displaymath}


\begin{displaymath}
  \ve{n}_k = \M{V}_{k-1}^\T \eve{q-k+1}{q},
  \quad
  \ve{q}_k
  = \frac{\eve{q-k+1}{q} - \M{V}_{k-1} \ve{n}_k}
  {\|  \eve{q-k+1}{q} - \M{V}_{k-1} \ve{n}_k\|}
  =
  \frac{\eve{q-k+1}{q} - \M{V}_{k-1} \ve{n}_k}{\sqrt{1 - \| \ve{n}_k \|^2}}
\end{displaymath}

\begin{align*}
  \|  \eve{q-k+1}{q} - \M{V}_{k-1} \ve{n}_k\|^2
  & =
    \| \eve{q-k+1}{q} \| ^2
    - 2 (\eve{q-k+1}{q})^\T \M{V}_{k-1}  \M{V}_{k-1}^\T \eve{q-k+1}{q}
  \\
  & \quad
    + \|  \M{V}_{k-1}  \M{V}_{k-1}^\T \eve{q-k+1}{q} \|^2
\end{align*}

\begin{displaymath}
  (\eve{q-k+1}{q})^\T \M{V}_{k-1}  \M{V}_{k-1}^\T \eve{q-k+1}{q}
  =  \ve{n}_k^\T  \ve{n}_k = \| \ve{n}_k \|^2
\end{displaymath}

\begin{displaymath}
  \|  \M{V}_{k-1}  \M{V}_{k-1}^\T \eve{q-k+1}{q} \|
  =
  \| \M{V}_{k-1}^\T \eve{q-k+1}{q} \| = \|  \ve{n}_k \|.
\end{displaymath}

\begin{displaymath}
  \M{K}_k
  =
  \begin{bmatrix}
    \M{S}_{k-1} & \0
    \\
    \0^\T & 0
  \end{bmatrix}
  \left(
    \M{I}
    -
    \begin{bmatrix}
      \ve{n}_k
      \\
      0
    \end{bmatrix}
    \begin{bmatrix}
      \ve{n}_k
      \\
      \sqrt{1 - \| \ve{n}_k \|^2}
    \end{bmatrix}
  \right)
\end{displaymath}

For each $1 \leq k \leq K$, the (reduced) SVD of $\M{K}_k$ takes the form
\begin{displaymath}
  \M{K}_k = \M{C}_k \M{S}_k  \M{D}_k^\T,
\end{displaymath}
where
\begin{align*}
  \M{C}_k
  & =
    \begin{bmatrix}
      \M{C}^{1:r_k, 1:r_k}_k
      \\
      \0^\T
    \end{bmatrix},
  \\[2ex]
  \M{S}_k & = \diag(s_{1,k}, \dots, s_{r_k,k}),
  \\[2ex]
  \M{D}_k & =
            \begin{bmatrix}
              \M{D}^{1:r_k, 1:r_k}_k
              \\
              \M{D}^{r_k+1, 1:r_k}_k
            \end{bmatrix}.
\end{align*}

Let
\begin{displaymath}
  \Mt{C}_1 = \M{C}^{1:r_0,:}_1
\end{displaymath}
and, for each $1 \leq k \leq K - 1$, let
\begin{displaymath}
  \Mt{C}_{k+1} = \Mt{C}_k \M{C}^{1:r_k, :}_{k+1}.
\end{displaymath}
For each $1 \leq k \leq K - 1$, let
\begin{displaymath}
  \M{U}_k = \M{U}_0 \Mt{C}_k
\end{displaymath}


Let
\begin{displaymath}
  \Mt{U}_1 = \M{U}_0
  \quad
  \text{and}
  \quad
  \Mt{C}_1 = \M{C}^{1:r_1, 1:r_1}_1
\end{displaymath}
and, for each $1 \leq k \leq K - 1$,
\begin{displaymath}
  \Mt{U}_{k+1} =  \Mt{U}_k
  \quad
  \text{and}
  \quad
  \Mt{C}_{k+1} = \Mt{C}_k \M{C}^{1:r_{k+1}, 1:r_{k+1}}_{k+1}.
\end{displaymath}




Let
\begin{displaymath}
  \Mt{V}_1 = [\M{V}_0, \ve{q}_1]
  \quad
  \text{and}
  \quad
  \Mt{D}_1 = \M{D}_1,
\end{displaymath}
and, for each $1 \leq k \leq K - 1$, let
\begin{displaymath}
  \Mt{V}_{k+1}
  =
  [\Mt{V}_k,  \ve{q}_{k+1}]
  \quad
  \text{and}
  \quad
  \Mt{D}_{k+1}
  =
  \begin{bmatrix}
    \Mt{D}_k & \0
    \\
    \0^\T & 1
  \end{bmatrix}
  \M{D}_{k+1}.
\end{displaymath}
Then
\begin{displaymath}
  \M{X}_k = \Mt{U}_k \M{S}_k \Mt{V}_k \Mt{D}_k.
\end{displaymath}

\end{comment}

\afterpage{\FloatBarrier}    

\bibliographystyle{mn}
% \bibliographystyle{unsrtnat}
\bibliography{references.bib}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% fill-column: 2000
%%% End:
